<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Support Vector Machines</title>
    <meta charset="utf-8" />
    <meta name="author" content="Héctor Corrada Bravo" />
    <meta name="date" content="2020-05-10" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Algorithms for Data Science: Support Vector Machines]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
CMSC644: 2020-05-10
]

.logo[![](img/logo.png)]

---

## Support Vector Machines

State-of-the-art classification and regression method

Flexible and efficient framework to learn classifers. 

Nice geometric interpretation of how they are trained (based maximum margin arguments). 

---

## Support Vector Machines

Can be estimated over _similarities_ between observations (more on this later) rather than standard data in tabular form. 

E.g., applications where string similarities, or network similarities are readily available. 

---
class: split-60

## Support Vector Machines

.column[SVMs follow the "predictor space partition" framework]



.column[
![](index_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
]

---

## Separating Hyperplanes

Training data: `\(\{(\mathbf{x}_1,y_1), (\mathbf{x}_2,y_2),\ldots,(\mathbf{x}_n,y_n)\}\)`


- `\(\mathbf{x}_i\)` is a vector of `\(p\)` predictor values for `\(i\)`th observation,  

- `\(y_i\)` is the class label (we're going to use +1 and -1) 

--

Build a classifier by defining
a _discriminative_ function such that

`$$w_0 + w_1 x_{i1} + w_2 x_{i2} + \cdots + w_p x_{ip} &gt; 0 \, \mathrm{ if } y_i = 1$$`

and

`$$w_0 + w_1 x_{i1} + w_2 x_{i2} + \cdots + w_p x_{ip} &lt; 0 \, \mathrm{ if } y_i = -1$$`

---

## Separating Hyperplanes

Points where the _discriminative_ function equals 0 form a _hyper-plane_ (i.e., a line in 2D)

`$$\{x \, : \, w_0 + w_1 x_{1} + \cdots + w_p x_p = 0 \}$$`

.center[.image-70[![](images/9_2.png)]]

---

## Separating Hyperplanes

Hyper-plane partitions the predictor space into two sets on
each side of the line. 

Denote `\(\mathbf{w}\)` is the vector `\((w_1, w_2, \ldots, w_p)\)`

--

Restrict estimates to those for which
`\(\mathbf{w}'\mathbf{w}=\|\mathbf{w}\|^2=1\)` 

--

Then, the *signed* distance of any point
`\(x\)` to the decision boundary `\(L\)` is `\(w_0 + \mathbf{w}'\mathbf{x}\)`. 

---

## Separating Hyperplanes

With this we can easily
describe the two partitions as

$$
`\begin{aligned}
L^+ &amp; =\{\mathbf{x}:w_0 + \mathbf{w}'\mathbf{x}&gt;0\}, \\\\
L^{-} &amp; =\{\mathbf{x}:w_0 + \mathbf{w}'\mathbf{x}&lt;0\}
\end{aligned}`
$$

---

## Separating Hyperplanes

The `\(\mathbf{w}\)` we want as an estimate is one that separates
the training data as perfectly as possible.

--

Describe this requirement as

$$
y_i(w_0 + \mathbf{w}'\mathbf{x}_i) &gt; 0, i=1,\ldots,N
$$

---

## Rosenblatt's algorithm

Algorithm to find vector `\(\mathbf{w}\)` that
satisfies the separation requirement as much as possible. 

Penalize `\(\mathbf{w}\)` by how far into the wrong side misclassified
points are:

`$$D(w_0, \mathbf{w}) = - \sum_{i\in \mathcal{M}} y_i (w_0 + \mathbf{w}'\mathbf{x}_i)$$`

`\(\mathcal{M}\)`: set of points misclassified  by `\(\mathbf{w}\)` (on the wrong side of the
hyper-plane).

---

## Rosenblatt's Algorithm

Estimate `\(\mathbf{w}\)` by minimizing `\(D\)`. 

Assuming `\(\mathcal{M}\)` is fixed, the gradient of `\(D\)` is

`$$\nabla_{\mathbf{w}}D(w_0,\mathbf{w}) = -\sum_{i\in \mathcal{M}} y_i \mathbf{x}_i$$`

and

`$$\frac{\partial D(w_0, \mathbf{w})}{\partial w_0} = -\sum_{i \in \mathcal{M}} y_i$$`

---

## Rosenblatt's Algorithm

Rosenblatt's algorithm uses *stochastic gradient descent*: 

- Initialize parameters `\(w_0\)` and `\(\mathbf{w}\)`
- Cycle through training points `\(i\)`, if it is misclassified, update parameters as

$$
\mathbf{w} \leftarrow \mathbf{w} + \rho y_i\mathbf{x}_i
$$

and

$$
w_0 \leftarrow w_0 + \rho y_i
$$

- Stop when converged (or get tired of waiting)
- ($rho$ is a learning rate parameter)
---

## Rosenblatt's Algorithm

Update Rule:

$$
\mathbf{w} \leftarrow \mathbf{w} + \rho y_ix_i
$$

Learning rate parameter `\(\rho\)` is used to control how much we update `\(\mathbf{w}\)` in each
step. 

--

This is the gradient descent algorithm that forms the basis of neural networks and deep learning. 

---

## Rosenblatt's Algorithm

There are a few problems with this algorithm:

If there exists `\(w_0\)` and `\(\mathbf{w}\)` that separates the training points perfectly, 

--

then there are an infinite number of `\(w_0\)` and `\(\mathbf{w}\)`s that also separate the data perfectly

---

## Rosenblatt's Algorithm

Algorithm will converge in a finite number of steps if the training data is separable

--

However, the number of finite steps can be *very* large

---

## Rosenblatt's Algorithm

When the training data is *not* separable, the algorithm will not converge.

---

## Support Vector Machines

Support Vector Machines (SVMs) are designed to directly
address these problems. 

---
class: split-50

## Support Vector Machines

.column[A central concept in SVMs that we did not see in logistic regression is **the margin**: the distance between the separating plane and its nearest datapoints.]

.column[
.image-90[![](images/9_3.png)]
]

---

## Support Vector Machines

When the data are separable, SVMs will choose the single
optimal `\(\mathbf{w}\)` that _maximizes_ the distance between the decision
boundary and the closest point in each class.

--

Why is this a good idea?

---

## Support Vector Machines

SVMs are designed from three _key insights_:

1. **Look for the maximum margin hyper-plane**
2. Only depend on pair-wise "similarities" of observations
3. Only depend on a subset of observations (support vectors)



Let's see these in turn.

---

## Maximum margin hyper-planes

Goal: find the hyper-plane  that separates training data with largest margin. 

This will tend to _generalize_ better since new observations have room to fall within margin and still be classified correctly. 

---

## Maximum margin hyper-planes

This can be cast as _optimization_ problem:

$$
\mathrm{max}_{w_0,\mathbf{w}} M \\\\
\mathrm{s.t.} \|\mathbf{w}\|^2 = 1 \\\\
y_i(w_0 + \mathbf{w}'\mathbf{x}_i) \geq M \, \forall i
$$


---

## Maximum margin hyper-planes

Rewrite optimization problem setting `\(M=1/\|\mathbf{w}\|^2\)` and using a little bit of algebra:

$$
\mathrm{min}_{w_0, \mathbf{w}} \frac{1}{2} \|\mathbf{w}\|^2 \\\\
\mathrm{s.t.} y_i(w_0 + \mathbf{w}'\mathbf{x}_i) \geq 1 \, \forall i
$$

---

## Maximum margin hyper-planes

$$
\mathrm{min}_{w_0, \mathbf{w}} \frac{1}{2} \|\mathbf{w}\|^2 \\\\
\mathrm{s.t.} y_i(w_0 + \mathbf{w}'\mathbf{x}_i) \geq 1 \, \forall i
$$

This is a _constrained_ optimization problem

Minimize the norm of `\(\mathbf{w}\)` under the constraint that it classifies every observation correctly. 

---

## Constrained Optimization

Recall the standard constrained optimization problem we encountered before

$$
`\begin{array}
{} \min_x &amp; f_0(x) \\
\mathrm{s.t.} &amp; f_i(x) \leq 0 \; i=1,\ldots,m\\
{} &amp; h_i(x) = 0 \; i=1,\ldots,p
\end{array}`
$$

--

And associated Lagrangian

`$$L(x,\lambda,\nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \sum_{i=1}^p \nu_i h_i(x)$$`

---
## Constrained Optimization

Let's define the _dual_ equation

$$
g(\lambda, \nu) = \min_x L(x,\lambda,\nu)
$$
--

And the dual problem

$$
`\begin{array}
{} \max_{\lambda, \nu} &amp; g(\lambda, \nu) \\
\mathrm{s.t.} &amp; \lambda_i \geq 0 \; i=1,\ldots,m
\end{array}`
$$

---

## Constrained Optimization

Duality gap: let `\(\tilde{x}\)` be primal optimal, then

$$
g(\lambda,\nu) \leq f_0(\tilde{x})
$$

--

If `\(\tilde{x}\)` is _primal optimal_ and `\((\tilde{\lambda},\tilde{\nu})\)` are _dual optimal_ then

$$
g(\tilde{\lambda},\tilde{\nu}) = f_0(\tilde{x})
$$
---

## Constrained Optimization

Let's restate the optimality conditions (Karush-Kuhn-Tucker) 

$$
`\begin{array}
{} f_i(\tilde{x}) \leq 0 &amp; (\textrm{primal feasible}) \\
h_i(\tilde{x}) = 0 &amp; {} \\
\tilde{\lambda}_i \geq 0 &amp; (\textrm{dual feasible}) \\
\tilde{\lambda_i}f_i(\tilde{x}) = 0 &amp; (\textrm{complementarity}) \\
\nabla_x L(\tilde{x},\tilde{\lambda},\tilde{\nu}) = 0 &amp; (\textrm{saddle point})
\end{array}`
$$

---

## Maximum-margin hyper-planes

What does the Lagrangian look like for the maximum-margin hyper-plane problem?

$$
L(w_0, \mathbf{w}, \mathbf{\alpha}) = \frac{1}{2} \|\mathbf{w} \|^2 + \sum_i -\alpha_i[y_i(w_0 + \mathbf{w}'\mathbf{x}_i) - 1)]
$$

--

You can then find _dual function_ by solving `\(\min_x L(x,\lambda,\nu)\)`

---

## Maximum-margin hyper-planes

In the maximum-margin hyper-plane case, the equivalent constrained maximization problem (the _dual_ problem) is:

`$$\mathrm{max}_{\mathbf{\alpha}} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k \mathbf{x}_i'\mathbf{x}_k \\\\
\mathrm{s.t.} \alpha_i \geq 0 \, \forall i$$`

---

## Maximum margin hyper-planes

`$$\mathrm{max}_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k \mathbf{x}_i'\mathbf{x}_k \\\\
\mathrm{s.t.} \alpha_i \geq 0 \, \forall i$$`

This quadratic optimization problem is usually easier to optimize than the original problem (notice there is only positivity constraints on `\(\alpha\)`).

--

We can still recover original parameters `\(w_0\)` and `\(\mathbf{w}\)` from solution `\(\alpha\)`.

---

## Support Vector Machines

_Key insight no. 2_: **SVMs only depend on pairwise "similarity" functions of observations**

`$$\mathrm{max}_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k \mathbf{x}_i'\mathbf{x}_k \\\\
\mathrm{s.t.} \alpha_i = 0 \, \forall i$$`

Only inner products between observations are required as opposed to the observations themselves.

---

## Support Vector Machines


Also, we can write the _discriminant_ function in equivalent form

`$$f(x) = w_0 + \sum_{i=1}^n y_i \alpha_i \mathbf{x}'\mathbf{x}_i$$`

--

Geometrically, we can think of the inner product between observations as a "similarity" measure. 

--

Therefore, we can fit these models with other measures that works as "similarities". 

---

## Support Vector Machines

_Key insight no. 3_: **SVMs only depend on a subset of observations (support vectors)**

Optimial solutions `\(\mathbf{w}\)` and `\(\alpha\)` must satisfy the following condition:

$$
\alpha_i [ y_i(w_0 + \mathbf{w}'\mathbf{x}_i) -1] = 0 \, \forall i.
$$

---

## Support Vector Machines

$$
\alpha_i [ y_i(w_0 + \mathbf{w}'\mathbf{x}_i) -1] = 0 \, \forall i.
$$

Case 1: `\(\alpha_i &gt; 0\)`, then the signed distance between observation `\(x_i\)` and the decision boundary is 1. 

This means that observation `\(\mathbf{x}_i\)` is _on the margin_

---

## Support Vector Machines

$$
\alpha_i [ y_i(w_0 + \mathbf{w}'\mathbf{x}_i) -1] = 0 \, \forall i.
$$

Case 2: `\(y_i (w_0 + \mathbf{w}'\mathbf{x}_i) &gt; 1\)`, then observation `\(x_i\)` is not on the margin and `\(\alpha_i=0\)`.

---

## Support Vector Machines

To define the discriminant function in terms of `\(\alpha\)`s we only need observations that are _on the margin_, 

i.e., those for which `\(\alpha_i &gt; 0\)`. 

--

These are called _support vectors_. 

--

Also implies we only need Support Vectors to make predictions.

---
class: split-30

## Non-separable data

.column[The method we have discussed so far runs into an important complication: 

_What if there is no separating hyper-plane?_. 
]

.column[![](images/9_6.png)]

---
class: split-30

## Non-separable data

The solution is to penalize observations on the **wrong side of the margin** by introducing _slack variables_ to the optimization problem.


`$$\mathrm{min}_{w_0,\mathbf{x},\xi} \; C\sum_{i=1}^N \xi_i + \frac{1}{2} \|\mathbf{w}\|^2 \\\\
\mathrm{s.t} \; y_i(w_0 + \mathbf{w}'\mathbf{x}_i) \geq 1 - \xi_i \, \forall i \\\\
\xi_i \geq 0 \, \forall i$$`

---

## Non-separable data

`$$\mathrm{min}_{w_0,\mathbf{w},\xi} \; C\sum_{i=1}^N \xi_i + \frac{1}{2} \|\mathbf{w}\|^2 \\\\
\mathrm{s.t} \; y_i(w_0 + \mathbf{w}'\mathbf{x}_i) \geq 1 - \xi_i \, \forall i \\\\
\xi_i \geq 0 \, \forall i$$`

`\(C\)` is a parameter that tradeoffs the width of the margin vs. the penalty on observations on the _wrong_ side of the margin. 

--

This is a "data fit + model complexity" learning objective.

---
class: split-30

## Non-separable data

.column[ 
`\(C\)` is a hyper-parameter to be selected by the user or via cross-validation model selection methods.]

.column[
.image-50[![](images/9_7.png)]
]

---

## Non-separable data

An elegant result is that this formulation doesn't change the dual problem we saw before very much:

`$$\mathrm{max}_{\alpha} \; \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k x_i'x_k \\\\
\mathrm{s.t.} \; 0 \leq \alpha_i \leq C \, \forall i$$`

---

## Non-separable data

Only need support vectors, where `\(\alpha_i &gt; 0\)` to define the discriminant function and make predictions. 

--

The smaller the cost parameter `\(C\)`, the learned SVM will have fewer support vectors. 

--

Think of the number of support vectors as a rough measure of the _complexity_ of the SVM obtained. 

---
class: split-50

## Non-linear Support Vector Machine

.column[What to do when we need non-linear partitions of predictor space to get a classifier?]

.column[
![](images/9_8.png)
]

---

## Non-linear Support Vector Machine

We can define the SVM discriminant function in terms of inner products of observations. 

We can generalize inner product using "kernel" functions that provide something like an inner product:

`$$f(\mathbf{x}) = w_0 + \sum_{i=1}^n y_i \alpha_i k(\mathbf{x}, \mathbf{x}_i)$$`


---
class: split-50

## Non-linear Support Vector Machine

.column[But, what is `\(k\)`? Let's consider two examples.

- _Polynomial kernel_: `\(k(\mathbf{x},\mathbf{x}_i) = 1+\langle \mathbf{x}, \mathbf{x}_i \rangle^d\)`

- _RBF (radial) kernel_: `\(k(\mathbf{x},\mathbf{x}_i) = \exp\{-\gamma \sum_{j=1}^p (x_{j}-x_{ij})^2\}\)`
]

.column[![](images/9_9.png)]

---

## Non-linear Support Vector Machine

&lt;img src="index_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

## Non-linear Support Vector Machine

The optimization problem is very similar

`$$\mathrm{max}_{\alpha} \; \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{k=1}^N \alpha_i \alpha_k y_i y_k k(\mathbf{x}_i,\mathbf{x}_k) \\\\
\mathrm{s.t.} \; 0 \leq \alpha_i \leq C \, \forall i$$`

---

## SVM classification example

Let's try fitting SVMs to a credit card default dataset. Let's start with a linear SVM (where `\(k\)` is the inner product). 

---

## SVM classification example

Here we are fitting three different SVMs resulting from using three different values of cost parameter `\(C\)`.



&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; cost &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; number_svs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; train_error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; test_error &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1e-02 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 350 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1e+00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 352 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1e+02 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 354 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.22 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

## SVM classification example

Let's try now a _non-linear_ SVM by using a radial kernel. 

Notice now that we have two parameters to provide to the fitting function: cost parameter `\(C\)` and parameter `\(\gamma\)` of the radial kernel function.

---

## SVM classification example




&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; cost &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; gamma &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; number_svs &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; train_error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; test_error &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.01 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.01 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 344 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.01 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 348 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.01 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 347 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.01 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 392 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 426 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.82 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.58 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 382 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.64 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.66 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.01 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 491 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.44 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1226 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.56 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.96 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 10.00 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 954 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.20 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.18 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

## Support Vector Machines

Different algorithms depending on data size

  - Massive number of examples with few predictors, train with stochastic gradient descent on the primal problem
  
  - Moderate number of examples, use quadratic optimization with kernel functions
  
  - For quadratic version, can subset observations that could be support vectors

---

## Support Vector Machines

State-of-the-art for many applications

RBF kernels usually work well, but tuning `\(\gamma\)` properly is very important

Very elegant formulation

Kernel trick gives a lot of flexibility
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>
<script>
remark.macros['scale'] = function (percentage) {
  var url = this;
  return '<img src="' + url + '" style=width: ' + percentage + '"/>';
};
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
