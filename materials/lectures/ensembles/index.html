<!DOCTYPE html>
<html>
  <head>
    <title>Ensemble Methods</title>
    <meta charset="utf-8">
    <meta name="author" content="Héctor Corrada Bravo" />
    <meta name="date" content="2018-10-23" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Ensemble Methods]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
CMSC 643: 2018-10-23
]

.logo[![](img/logo.png)]

---

# Ensemble Learning

In studying the Random Forest algorithm we have seen some advantages of aggregating models to improve our predictions. 

--

This idea of combining models is called _Ensemble Learning_ in general. 

--

At a high level, _Ensemble Learning_ is _the aggregation of predictions of multiple weak learners with the goal of improving prediction performance._

---

## Ensemble Learning

We will see three general approaches to this methodology: 

--

**bagging**: uses ensembles to reduce the variability of single ML models, 

--

**stacking**: uses ensembles to capture different characteristics of task, learning how to combine them

--

**boosting**: uses ensembles of ML models each capturing a specific subspace of predictor space. 

---

## The story of the Netflix Prize

In October 2006 Netflix announced an ML prize around their movie recommendation engine. 

--

Supervised learning task: 

- Dataset of users and their ratings, (1,2,3,4 or 5 stars), of movies they have rated. 
- Build an ML model that given predicts a specific user's rating to a movie they have not rated. 

--

The idea is that they can then recommend movies to those users if they predict they would rate them highly.

---

## The story of the Netflix Prize

Netflix would award $1M for the first ML system that provided a 10% improvement to their existing system

---
class: split-30

## The story of the Netflix Prize

.column[
Existing system had a 0.9514 mean squared error
]

.column[
.image-60[![Netflix Challenge 3 week leaderboard](images/netflix1.png)
]]

---
class: split-30

## The story of the Netflix Prize

.column[
Within three weeks, at least 40 teams had improved upon the existing Netflix system. 

The top teams were showing improvement over 5%. 
]

.column[
.image-60[![Netflix Challenge 3 week leaderboard](images/netflix1.png)
]]

---
class: split-50

## The story of the Netflix Prize

.column[
Progress soon slowed and teams were stuck at around 8-9% improvement over the existing system.
]

.column[
.image-50[![Netflix Challenge progress](images/netflix2.png)]]

---
class: split-50

## The story of the Netflix Prize


.column[Along the way, progress prizes were awarded based on the top team at each challenge anniversary. 

The top teams were using ensemble methods.]

.column[
.image-50[![Netflix Challenge Progress Prize](images/netflix3.png)]]

---
class: split-50

## The story of the Netflix Prize

.column[
- **Arek Paterek**: "combine the results of many methods"  
- **U of Toronto**: "when the predictions of multiple RBM and SVD models are linearly combined..."
]

.column[
.image-50[![Netflix Challenge Progress Prize](images/netflix3.png)]]

---
class: split-50

## The story of the Netflix Prize

.column[
- **When Gravity and Dinosaurs Unite**: "Our common team blends the result of team Gravity and team Dinosaur Planet"  
- **BellKor**: "Our final solution consists of blending 107 individual resluts"
]

.column[
.image-50[![Netflix Challenge Progress Prize](images/netflix3.png)]]

---
class: split-30

## The story of the Netflix Prize

.column[
Ultimately, the challenge was won when multiple top teams allied to combine their own ensemble models as ensembles. 
]

.column[
.image-70[![Netflix Challenge Final Leaderboard](images/netflix4.png)]]

---

## Intuition behind ensemble methods

So what is the intuition behind the success of these ensemble models?

--

First, there is the general protective mechanism of diversification. 

--

For instance, combining diverse independent opinions in human decision-making. 

--

Averting risk by diversifying a stock portfolio.

---

## Intuition behind ensemble methods

So what is the intuition behind the success of these ensemble models?

Second, it is generally difficult to establish precisely the type and complexity of model required for a specific learning task. 

--

A combination of models of diverse types and complexities can alleviate this challenge.

---

## Intuition behind ensemble methods

Here is another, mathematical intuition behind model combination. 

--

Suppose we have completely independent classifiers, each having 70% accuracy on a specific task. 

--

Now, suppose we combine them using majority vote, where each classifiers predicts a label for an instance, and we make a final prediction based on what the majority of classifiers predicted. 

---
class:split-50

## Intuition behind ensemble methods

.column[In this case, the accuracy of the ensemble increases as the number of classifiers increase. 

In this case, we would reach 99% accuracy with about 40 classifiers.]

.column[
&lt;img src="index_files/figure-html/ch4_binom_ensemble-1.png" width="480" /&gt;
]

---

## Ensemble Models

We will look at three strategies for building ensembles.

Bagging: The goal is to construct diverse models. 

--

Use different samples of instances and/or attributes to independently train diverse classifiers. 

--

Then, aggregate the classifiers using majority vote (classification) or averaging (regression).

---
## Ensemble Models

Stacking: Build ensembles in parallel

--

Different model types, different features, lots of diversity

--

Learn how to combine models via a simple blending model

---
## Ensemble Models

Boosting: Build the ensemble sequentially, using the performance of the ensemble to train the next classifier in the ensmeble.

---

## Bagging

The name Bagging comes from combining the _bootstrap_ to generate different samples of instances and _aggregation_ used to combine predictions. 

--

This method increases diversity of the weak learners using randomness in two ways

1. using bootstrap sampling to generate the training set for each weak learner  
2. using random subsets of features to train each weak learner


---

## Bagging

The general bootstrap algorithm was designed to estimate variability of estimates when we do not have support for a specific data generating model. 

---

## Bagging

The general bootstrap procedure is as follows for parameter `\(\theta\)` and estimator `\(s\)`:

1. Select `\(B\)` independent bootstrap samples `\(\mathbf{x}_1^*, \ldots, \mathbf{x}_B^*\)` each consisting of `\(N\)` data values drawing _with replacement_ from training set `\(\mathbf{x}\)`.  

2. Evaluate each bootstrap replication to estimate `\(\hat{\theta}(b) = s(\mathbf{x}_b^*)\)`, for `\(b=1,\ldots,B\)`.

3. Estimate standard error of `\(\hat{\theta}\)` using the sample standard error of the `\(B\)` `\(\hat{\theta}\)` estimates.

---

## Bagging

In the ensemble learning case, `\(s\)` is an ML training algorithm 

--

`\(\hat{\theta}_b\)` is interpreted as the predictions made by the ML algorithm trained using bootstrap sample `\(b\)`. 

--

In this case we are not necessarily interested in the inferential task. 

--

Instead, we use aggregation of these multiple predictions to make final predictions.

---

## Bagging

Bagging  works best when perturbing the training data can cause significant changes in the estimated model. 

--

This is specially notable in decision trees. 

--

For some models, like linear regression models, we can show anatically that this strategy decreases variance without increasing bias.

---
layout: true

## Stacking

---

A related idea is stacking (also known as blending)

This is what the Netflix teams did as they combined predictors near the end of the challenge.

---
class: split-50

The basic idea is to train a set of diverse predictors as done in bagging with some differences:

.column[
Train each model (e.g. SVM, Decision Tree, KNN) on the entire set of training observations (instead of bootstrapped samples)

Train a simple model (e.g., linear regression or logistic regression) using the _predictions_ made by the predictors as _features_

]

.column[
.image-50[![](images/stacking.png)]
]

---
layout: false

## Geometric Representation of Classification Problems

Let's consider again little bit how we think of training data. For each instance `\(i\)`:

- predictors (covariates) `\(x_i\)`, 
- *qualitative* outcomes (or classes) `\(g_i\)`, which can take
values from a discrete set `\(G\)`.

--

We can always divide the input space into a
collection of regions taking the same predicted values.

---
class: split-50


.column[
Boundaries can be linear or non-linear depending
on the _decision_ function. 
]

.column[![decision boundaries](images/classification-plots-01.png)]

---

### Linear expansions

It will be helpful throughout our discussion to represent classification in terms of decision functions

`$$f(\mathbf{x}_i) = \sum_{m=1}^M h_m(\mathbf{x}_i)$$`

For binary classification a single decision function is needed, the sign of `\(f(\mathbf{x}_i)\)` is used to choose **positive** or **negative**

---
exclude: true
layout: false
class: split-50

## Generalized Additive Model (GAM)

.column[
Make each term `\(h_m(\mathbf{x}_i)\)` a non-linear function (spline) of 1 or 2 predictors

- Still interpretable (easy to plot outcome vs. term)
- Still probability
- Which interactions?
]

.column[
.image-50[![](images/gams.pdf)]
]

---
class: split-50

## MARS (multivariate adaptive regression spline)

.column[
Make each term a simple form:

`$$h_m(\mathbf{x}_i) = \beta_m (x_{im}-t_{m})_+$$`
(and products of pairs these)

- Still (somewhat interpretable)
- Which terms to add? **Algorithmic Search**
]

.column[
.center[.image-120[![](images/mars.pdf)]]
]

---

## Forward Stagewise Additive Modeling

**Algorithm**

(1) Initialize `\(f_0(\mathbf{x})=0\)`  
(2) For `\(m=1,\ldots,M\)`:  
  (a) Compute
  
  `$$(\beta_m, t_m) = \arg \min_{\beta,t} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \beta b(x_i; t))$$`
  
  (b) Set `\(f_m = f_{m-1}(x) + \beta_m b_m(x; t_m)\)`
  
---

## Forward Stagewise Additive Modeling

Computing the next term:

Suppose we use _least squares_ as the loss function:

`$$L(y_i,f_{m-1}(x_i) + \beta b(x_i; t)) = (y_i - (f_{m-1}(x_i) + \beta b(x_i; t)))^2$$`

Then we are minimizing 

`$$\sum_{i=1}^N (r_i - \beta b(x_i; t))^2$$`

where `\(r_i\)` is the **residual** of the model at stage `\(m-1\)`.

---

## Forward Stagewise Additive Modeling

For other loss functions (e.g., negative log binomial likelihood, i.e., logistic regression)

Minimize

`$$\sum_{i=1}^N (-g_i - \beta b(x_i; t))^2$$`

where `\(g_i\)` is the _gradient_ of the loss function for instance `\(i\)`

`$$g_i = \left[ \frac{\partial L(y_i, f(x_i))}{\partial f(x_i)} \right]_{f(x_i) = f_{m-1}(x_i)}$$`

---
layout: false

## Boosting

Create an ensemble of classifiers sequentially. 

At each iteration of the sequence we modify the loss function so that instances that are incorrectly classified by the current set of classifiers are given higher weight. 

--

Here diversity is injected into the ensemble by sequentially "re-weighting" training instances. 

---

## Boosting

Final prediction from the ensemble is also given by aggregation (majority decision or averaging) 

--

Predictions are weighted based on each classifiers accuracy. 

---

## Adaboost

The Adaboost algorithm is probably the best known example of boosting. The algorithm is as follows:

- Initialize weights: each instance gets the same weight

$$
w_i = 1/N, i = 1, \ldots, N
$$


- Construct a classifier `\(m\)` using current weights (Decision Trees, SVM, linear/logistic regression). Compute the error of the new classifier

$$
\epsilon_m = \frac{\sum_i w_i I\{y_i \neq g_m(x_i)\}}{\sum_i w_i}
$$

---

## Adaboost

- Get the _influence_ of the new classifier and update training instance weights

$$
\alpha_m = \log \left( \frac{1-\epsilon_i}{\epsilon_i}\right) \\\\
w_i \leftarrow w_i \exp\{\alpha_m I\{y_i \neq g_m(x_i)\}\}
$$

- Goto step 2

For final prediction, average predictions from each classifier with weight `\(\alpha_m\)`. 

---
class: middle, center

## Adaboost

.image-70[![Adaboost illustration](images/adaboost.png)]

---

## Adaboost

Adaboost has the advantage of its simplicity 

--

Like bagging, adaboost reduces the variability of individual weak learners. 

--

Unlike bagging it is sensitive to noise and outliers.

---

## Gradient Boosted Trees

Uses the sequential additive modeling idea we saw previously. At each iteration learn a regression tree `\(T(x)\)` to minimize 

`$$\sum_{i=1}^N (-g_i - T(x_i))^2$$`

Very close to Adaboost if loss is "exponential": `\(L(y_i,f_i)=\exp\{-y_if_i\}\)`.

Not a robust loss function to use, prefer binomial deviance (i.e., logistic regression negative log likelihood)

---

## Gradient Boosted Trees

Excellent software XGBoost: most commonly generally

Actually minimizes a regularized loss function: no need to prune trees

Can still compute variable importance: 
  - compute variable importance for each tree
  - add variable importance across trees
  
No simple interpretation by inspection, but there are options...

---
layout: false

## Model explanation 

.center[.image-120[![](images/explanation.png)]]

- Surrogate models
- Local explanatory models

---
class: split-50

## Model explanation

.column[
Use a _sparse_ simple model (e.g., logistic regression)
to explain the prediction of a complicated model for a specific instance `\(i\)`

LIME: https://github.com/marcotcr/lime  
ELI5: http://eli5.readthedocs.io/en/latest/index.html
]

.column[
.center[![](images/lime.png)]
]

---

## Model explanation

![](images/lime2.png)

LIME: https://github.com/marcotcr/lime  
ELI5: http://eli5.readthedocs.io/en/latest/index.html

---

## Summary

Ensemble methods seek to 

1. reduce variance of individual weak learners by aggregating their predictions. 

2. improve performance by exploiting prediction diversity

--

Bagging, boosting and stacking are alternative methods of training ensembles while providing diversity to each of the trained weak learners.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>
<script>
remark.macros['scale'] = function (percentage) {
  var url = this;
  return '<img src="' + url + '" style=width: ' + percentage + '"/>';
};
</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
