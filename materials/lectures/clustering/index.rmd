---
title: "Data Clustering"
author: "Héctor Corrada Bravo"
company: "University of Maryland"
date: "`r Sys.Date()`"
css: ["custom.css"]
output: 
  xaringan::moon_reader:
    lib_dir: libs
    seal: false
    includes:
      after_body: "custom.html"
    nature:
      ratio: "16:9"
---
class: title-slide, center, middle
count: false

```{r cowplot_setup, echo=FALSE, message=FALSE}
library(cowplot)
```

.banner[![](img/epiviz.png)]

.title[Data Clustering]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
DATA606: `r Sys.Date()`
]

.logo[![](img/logo.png)]

---

## Motivating Example

Time series dataset of mortgage affordability
as calculated and distributed by Zillow: https://www.zillow.com/research/data/. 

```{r clustering_setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(readr)
library(lubridate)

datadir <- "data"
url <- "http://files.zillowstatic.com/research/public/Affordability_Wide_2017Q1_Public.csv"
filename <- basename(url)
datafile <- file.path(datadir, filename)

if (!file.exists(datafile)) {
  download.file(url, file.path(datadir, filename))
}

afford_data <- read_csv(datafile)
```

```{r tidy_zillow, echo=FALSE, cache=TRUE}
tidy_afford <- afford_data %>%
  filter(Index == "Mortgage Affordability") %>% 
  drop_na() %>%
  filter(RegionID != 0) %>%
  dplyr::select(RegionID, matches("^[1|2]")) %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m")))

wide_afford_df <- tidy_afford %>%
  dplyr::select(RegionID, time, affordability) %>%
  spread(time, affordability)

value_mat <-  wide_afford_df %>%
  dplyr::select(-RegionID) %>%
  as.matrix() 
```

```{r zillow_stats, echo=FALSE}
ncounties <- nrow(wide_afford_df)
year_range <- range(year(tidy_afford$time))
```

The dataset consists of monthly mortgage affordability values for `r ncounties` counties with data from `r min(year_range)` to `r max(year_range)`. 

---

## Motivating Example

> "To calculate mortgage affordability, we first calculate the mortgage payment for the median-valued home in a metropolitan area by using the metro-level Zillow Home Value Index for a given quarter and the 30-year fixed mortgage interest rate during that time period, provided by the Freddie Mac Primary Mortgage Market Survey (based on a 20 percent down payment)."

---

## Motivating Example

> "Then, we consider what portion of the monthly median household income (U.S. Census) goes toward this monthly mortgage payment. Median household income is available with a lag. "

---
class: split-60

## Motivating Example

.column[
```{r zillow_plot1, echo=FALSE, cache=FALSE, warning=FALSE, fig.align="center"}
tidy_afford %>%
  ggplot(aes(x=time,y=affordability,group=factor(RegionID))) +
  geom_line(color="GRAY", alpha=3/4, size=1/2) +
  labs(title="County-Level Mortgage Affordability over Time",
          x="Date", y="Mortgage Affordability")
```
]

.column[Can we partition counties into groups of counties with similar value trends across time?]

---

## Cluster Analysis

The high-level goal of cluster analysis is to organize objects (observations) that are _similar_ to each other into groups. 

We want objects within a group to be more _similar_ to each other than objects in different groups. 

--

Central to this high-level goal is how to measure the degree of _similarity_ between objects. 

A clustering method then uses the _similarity_ measure provided to it to group objects into clusters.

```{r setup_kmeans, echo=FALSE, warning=FALSE, message=FALSE}
library(broom)
library(stringr)
```

```{r kmeans1, cache=FALSE, echo=FALSE}
set.seed(1234)
kmeans_res <- kmeans(value_mat, centers=9)

augmented_data <- kmeans_res %>%
  broom::augment(wide_afford_df) %>%
  gather(time, affordability, matches("^X")) %>%
  mutate(time=stringr::str_replace(time, "X", "")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y.%m.%d"))) %>%
  rename(cluster=.cluster)
        
kmeans_centers <- kmeans_res %>%
  broom::tidy(col.names=colnames(value_mat)) %>%
  dplyr::select(cluster, matches("^[1|2]")) %>%
  gather(time, affordability, -cluster) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m-%d")))
```

---
class: split-50

## Cluster Analysis

.column[
```{r kmeans_plot, echo=FALSE, cache=TRUE, fig.height=6}
augmented_data %>%
  ggplot(aes(x=time, y=affordability)) +
    geom_line(aes(group=RegionID), color="GRAY", alpha=1/2, size=1/2) +
    facet_wrap(~cluster) +
    geom_line(data=kmeans_centers, color="BLACK", alpha=1/2, size=1/2) +
  labs(main="Kmeans Clustering (k=9)",
       xlab="Date", ylab="affordability") +
  theme(axis.text.x=element_text(angle=45, hjust=1))
```
]

.column[Result of the k-means algorithm partitioning the data into 9 clusters.

The darker series within each cluster shows the average time series within the cluster.
]

---

## Dissimilarity-based Clustering

For certain algorithms, instead of similarity we work with dissimilarity, often represented as distances. 

When we have observations defined over attributes, or predictors, we define dissimilarity based on these attributes. 

---

## Dissimilarity-based Clustering

Given measurements $x_{ij}$ for $i=1,\ldots,N$ observations over $j=1,\ldots,p$ predictors. 

Suppose we define a dissimilarity $d_j(x_{ij}, x_{i'j})$, we can then define a dissimilarity between objects as

$$d(x_i, x_{i'}) = \sum_{j=1}^p d_j(x_{ij},x_{i'j})$$

---

## Dissimilarity-based Clustering

In the k-means algorithm, and many other algorithms, the most common usage is squared distance

$$d_j(x_{ij},x_{i'j}) = (x_{ij}-x_{i'j})^2$$

We can use different dissimilarities, for example

$$d_j(x_{ij}, x_{i'j}) = |x_{ij}-x_{i'j}|$$

which may affect our choice of clustering algorithm later on. 

---

## Dissimilarity-based Clustering

For categorical variables, we could set 

$$d_j(x_{ij},x_{i'j}) = \begin{cases}
0 \; \textrm{if } x_{ij} = x_{i'j} \\\\
1 \; \textrm{o.w.}
\end{cases}$$


---

## Dissimilarity-based Clustering

If the values the categorical variable have an intrinsic similarity

Generalize using symmetric matrix $L$ with elements 

$L_{rr'} = L_{r'r}$,  
$L_{rr}=0$ and   
$L_{rr'} \geq 0$ otherwise. 

This may of course lead to a dissimilarity that is not a proper distance.

---

## Limitations of Distance-based clustering

When working with distances, we are quickly confronted with the _curse of dimensionality_.

One flavor: in high dimensions: all points are equi-distant

---
layout: true

## The curse of dimensionality

---

Consider the case where we have many covariates. We want to use a distance-based clustering  method.  

--

Basically, we need to define distance and look for small
multi-dimensional "balls"
around the data points. With many covariates this becomes
difficult. 

---


Imagine we have equally spaced data and that each
covariate is in $[0,1]$. We want something that clusters points into _local_ regions, containg some reasonably small amount of data points (say 10%). Let's imagine these are high-dimensional cubes.

--

If we have 
$p$ covariates and we are forming $p$-dimensional cubes, then each
side of the cube must have size $l$ determined by $l \times l \times \dots \times l = l^p = .10$. 

---

If the number of covariates is p=10, then $l = .1^{1/10} = .8$. So it
really isn't local! If we reduce the percent of data we consider to
1%, $l=0.63$. Still not very local.  

--

If we keep reducing the size of the 
neighborhoods  we will end up with very small number
of data points in each cluster and require a large number of clusters.

---
layout: false

## K-means Clustering

A commonly used algorithm to perform clustering is the K-means algorithm. 

It is appropriate when using squared Euclidean distance as the measure of object dissimilarity.

$$\begin{aligned} d(x_{i},x_{i'}) & = \sum_{j=1}^p (x_{ij}-x_{i'j})^2 \\\\
{} & = \|x_i - x_{i'}\|^2
\end{aligned}$$

---

## K-means Clustering

K-means partitions observations into $K$ clusters, with $K$ provided as a parameter. 

Given some clustering, or partition, $C$, denote cluster assignment of observation $x_i$ to cluster $k \in \{1,\ldots,K\}$ is denoted as $C(i)=k$. 

--

K-means minimizes this clustering criterion:

$$W(C) = \frac{1}{2} \sum_{k=1}^K \sum_{i: \, C(i)=k} \sum_{i':\, C(i')=k} \|x_i - x_{i'}\|^2$$

---

## K-means Clustering

This is equivalent to minimizing

$$W(C) = \frac{1}{2}\sum_{k=1}^K N_k \sum_{i:\,C(i)=k} \|x_i - \bar{x}_k\|^2$$

with:

- $\bar{x}_k=(\bar{x}_{k1},\ldots,\bar{x}_{kp})$   
- $\bar{x}_{kj}$ is the average of predictor $j$ over the observations assigned to cluster $k$, 
- $N_k$ is the number of observations assigned to cluster $k$

---

## K-means Clustering

$$W(C) = \frac{1}{2}\sum_{k=1}^K N_k \sum_{i:\,C(i)=k} \|x_i - \bar{x}_k\|^2$$

Minimize the total distance given by each observation to the mean (centroid) of the cluster to which the observation is assigned.

---

## K-means Clustering

An iterative algorithm is used to minimize this criterion

1. Initialize by choosing $K$ observations as centroids $m_1,m_2,\ldots,m_k$
2. Assign each observation $i$ to the cluster with the nearest centroid, i.e., set $C(i)=\arg\min_{1 \leq k \leq K} \|x_i - m_k\|^2$
3. Update centroids $m_k=\bar{x}_k$
4. Iterate steps 2 and 3 until convergence

---
class: split-30

## K-means Clustering

.column[
Here we illustrate the k-means algorithm over four iterations on our example data with $K=4$. 
]

```{r kmeans_illustration, echo=FALSE, message=FALSE, cache=TRUE}
set.seed(1234)

K <- 4
nobs <- nrow(value_mat)
centroid_indices <- sample(nobs, K)
centroids <- value_mat[centroid_indices,]

assign_cluster <- function(x, m) {
#  browser()
  xx <- rowSums(x^2)
  mm <- rowSums(m^2)
  xm <- tcrossprod(x, m)
  
  d <- xx - 2*xm
  d <- sweep(d, 2, mm, FUN="+")
  apply(d, 1, which.min)
}

get_centroids <- function(x, a) {
  inds <- split(seq_len(nobs), a)
  sapply(inds, function(i) colMeans(x[i,,drop=FALSE])) %>%
    t()
}

message("Iteration 1")
assignments <- assign_cluster(value_mat, centroids)

centroid_df <- as.data.frame(cbind(centroids, cluster=seq_len(K), iteration=1))

assignments_df <- cbind(wide_afford_df, cluster=assignments, iteration=1)

for (it in seq(2,4)) {
  message("Iteration ", it)
  centroids <- get_centroids(value_mat, assignments)
  assignments <- assign_cluster(value_mat, centroids)
  
  tmp <- as.data.frame(cbind(centroids, cluster=seq_len(K), iteration=it))
centroid_df <- rbind(centroid_df, tmp)

  tmp <- cbind(wide_afford_df, cluster=assignments, iteration=it)
assignments_df <- rbind(assignments_df, tmp)
}
```

.column[
```{r kmeans_illustration2, echo=FALSE, cache=TRUE}
tall_assignments_df <- assignments_df %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m-%d"))) 

tall_centroids_df <- centroid_df %>% 
  gather(time,affordability,matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m-%d")))

pl <- tall_assignments_df %>%
  ggplot(aes(x=time, y=affordability)) +
    geom_line(aes(group=RegionID), color="GRAY", alpha=1/2, size=1/2) + 
    facet_grid(iteration~cluster) +
    geom_line(data=tall_centroids_df, color="BLACK") +
    labs(xlab="Date", ylab="mortgage affordability") +
    theme(axis.text.x=element_text(angle=45, hjust=1))
show(pl)
```
]
---

## K-means Clustering

Criterion $W(C)$ is reduced in each iteration so the algorithm is assured to converge. 

Not a convex criterion, the clustering we obtain may not be globally optimal. 

In practice, the algorithm is run with multiple initializations (step 0) and the best clustering achieved is used. 

---

## K-means Clustering

Also, selection of observations as centroids can be improved using the K-means++ algorithm:

0. Choose an observation as centroid $m_1$ uniformly at random  
1. To choose centroid $m_k$, compute for each observation $i$ not chosen as a centroid the distance to the nearest centroid $d_i = \min_{1\leq l < k} \|x_i - m_l\|^2$
2. Set centroid $m_k$ to an observation randomly chosen with probability $\frac{e^d_i}{\sum_{i'} e^d_{i'}}$
3. Iterate steps 1 and 2 until $K$ centroids are chosen

---

## Choosing the number of clusters

The number of parameters must be determined before running the K-means algorithm. 

There is no clean direct method for choosing the number of clusters to use in the K-means algorithm (e.g. no cross-validation method)

```{r gapstat, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, results="hide"}
set.seed(1234)
gap_stat <- cluster::clusGap(value_mat, FUN=kmeans, nstart=15, B=100, K.max=9)

gap_stat_df <- gap_stat$Tab %>%
  as_tibble() %>%
  rowid_to_column("k")
```

---
class: split-30

## Choosing the number of clusters

.column[
Looking at criterion $W(C)$ alone is not sufficient as the criterion will become smaller as the value of $K$ is reduced. 
]

.column[
```{r logw_plot, echo=FALSE}
gap_stat_df %>%
  ggplot(aes(x=k, y=logW)) +
    geom_line() +
    geom_point()
```
]

---

## Choosing the number of clusters

We can use properties of this plot for ad-hoc selection.

Suppose there is a true underlying number $K^*$ of clusters in the data, 

- improvement in the $W_K(C)$ statistic will be fast for values of $K \leq K^*$
- slower for values of $K > K^*$. 

---

## Choosing the number of clusters

_Improvement in the $W_K(C)$ statistic will be fast for values of $K \leq K^*$_

In this case, there will be a cluster which will contain observations belonging to two of the true underlying clusters, and therefore will have poor within cluster similarity. 

As $K$ is increased, observations may then be separated into separate clusters, providing a sharp improvement in the $W_K(C)$ statistic. 

---

## Choosing the number of clusters

_Improvement in the $W_K(C)$ statistic will be slower for values of $K > K^*$_

In this case, observations belonging to a single true cluster are split into multiple cluster, all with generally high within-cluster similarity, 

Splitting these clusters further will not improve the $W_K(C)$ statistic very sharply. 

---
class: split-30

## Choosing the number of clusters

.column[The curve will therefore have an inflection point around $K^*$.
]

.column[
```{r logw_plot_again, echo=FALSE}
gap_stat_df %>%
  ggplot(aes(x=k, y=logW)) +
    geom_line() +
    geom_point()
```
]

---

## Choosing the number of clusters

The _gap statistic_ is used to identify the inflection point in the curve. 

It compares the behavior of the $W_K(C)$ statistic based on the data with the behavior of the $W_K(C)$ statistic for data generated uniformly at random over the range of the data. 

Chooses the $K$ that maximizes the gap between these two $W_K(C)$ curves.

---
class: split-30

## Choosing the number of clusters

.column[For this dataset, the gap statistic suggests there is no clear cluster structure and therefore $K=1$ is the best choice.

A choice of $K=4$ is also appropriate.]

.column[
```{r gapstat_plot, echo=FALSE}
factoextra::fviz_gap_stat(gap_stat)
```
]

---

## K-medioids clustering

A variant of the same algorithm for distance that are not Euclidean

Input: Distance matrix $d(x_i, x_k)$ between observations  
Output: Cluster assignments, and _a representative data point_ for each cluster (medioid)  

Advantage: Can apply to situations where distances between observations are available but not feature vectors (e.g., network data)

---

## K-medioids clustering

1. Initialize by choosing $K$ observations as medioids $m_1,m_2,\ldots,m_k$
2. Assign each observation $i$ to the cluster with the nearest medioid, i.e., set $C(i)=\arg\min_{1 \leq k \leq K} d(x_i, m_k)$
3. Update medioids $m_k=\arg\min_{x_i \textrm{ s.t. } C(i)=k} \sum_{j \textrm{ s.t. } C(j)=k} d(x_i, x_j)$
4. Iterate steps 2 and 3 until convergence

---

## Large-scale clustering

Cost of K-means as presented:

Each iteration: Compute distance for each point to centroid $O(knp)$
  
--

This implies we have to do multiple passes over entire dataset.

Not good for massive datasets

--

Can we do this in "almost" a single pass?

---
layout: true

## Large-scale clustering (BFR Algorithm)

---

1. Select $k$ points as before
2. Process data file in chunks:
  - Set chunk size so each can be processed in main memory
  - Will use memory for workspace so not entire memory available
  
---

For each chunk

- All points _sufficiently_ close to the centroid of one of the $k$ clusters is assigned to that cluster (_Discard Set_)

--

- Remaining points are clustered (e.g., using $k$-means with some value of $k$. Two cases
  - Clusters with more than one point (_Compressed Set_)
  - Singleton clusters (_Retained Set_)

--

- Try to merge clusters in _Compressed Set_

---

What is sufficiently close?

- "Weighted" distance to centroid below some threshold.

$$
\sqrt{\sum_{i=1}^d\frac{(p_i - c_i)^2}{\sigma_i}}
$$
- $c_i$: cluster mean for feature $i$  
- $\sigma_i$: cluster standard deviation of feature $i$

---

### Assumption: points are distributed along axis-parallel ellipses

![](img/ok.png)

---

Under this assumption, we only need to store means and variances to calculate distances

--

We can do this by storing for each cluster $j$:

- $N_j$ number of points assigned to cluster  
- $s_{ij}$ sum of values of feature $i$ in cluster $j$   
- $s^2_{ij}$ sum of squares of values of feature $i$ in cluster $j$

--

Constant amount of space to represent cluster

--

_Exercise._ show these are sufficient to calculate weighted distance

---

This is used to represent (final) clusters in _Discard Set_ and (partial) clusters in _Compressed Set_

Only points explicitly in memory are those in the _Retained Set_

--

Points outside of _Retained Set_ are never kept in memory (written out along with cluster assignment)

---

### Merging clusters in _Compressed Set_

Example: Merge if the variance of combined clusters is sufficiently close to variance of separate clusters

---

After all data is processed:

- Assign points in _Retained Set_ to cluster with nearest centroid

- Merge partial clusters in _Compressed Set_ with final clusters in _Discarded Set_

--

Or,

Flag all of these as _outliers_

---
layout: false
exclude: true

## Soft K-means Clustering

Instead of the combinatorial approach of the $K$-means algorithm, take a more direct probabilistic approach to modeling distribution $Pr(X)$. 

Assume each of the $K$ clusters corresponds to a multivariate distribution $Pr_k(X)$, 

$Pr(X)$ is given by _mixture_ of these distributions as $Pr(X)=\sum_{k=1}^K \pi_k Pr_k(X)$. 

---
exclude: true
## Soft K-means Clustering

Specifically, take $Pr_k(X)$ as a multivariate normal distribution $f_k(X) = N(\mu_k, \sigma_k^2 I)$ 

and mixture density 
$f(X) = \sum_{k=1}^K \pi_k f_k(X)$. 

---
exclude: true
## Soft K-means Clustering

Use Maximum Likelihood to estimate parameters
$$\theta=(\mu_1, \ldots,\mu_K,\sigma_1^2,\ldots,\sigma_K^2,\pi_1,\ldots,\pi_K)$$ 

based on their log-likelihood
$$\ell(\theta;X) = \sum_{i=1}^N \log \left[ \sum_{k=1}^K \pi_k f_k(x_i;\theta) \right]$$

---
exclude: true

## Soft K-means Clustering

$$\ell(\theta;X) = \sum_{i=1}^N \log \left[ \sum_{k=1}^K \pi_k f_k(x_i;\theta) \right]$$

Maximizing this likelihood directly is computationally difficult

Use Expectation Maximization algorithm (EM) instead. 

---
exclude: true

## Soft K-means Clustering

Consider unobserved latent variables $\Delta_{ik}$ taking values 0 or 1, 

$\Delta_{ij}=1$ specifies observation $x_i$ was generated by component $k$ of the mixture distribution. 
---
exclude: true

## Soft K-means Clustering

Now set $Pr(\Delta_{ik}=1)=\pi_{k}$,and assume we _observed_ values for indicator variables $\Delta_{ik}$. 

We can write the log-likelihood of our parameters in this case as

$$\ell_0(\theta; X, \Delta) = \sum_{i=1}^N \sum_{k=1}^K \Delta_{ik} \log f_k(x_i; \theta) + \sum_{i=1}^N \sum_{k=1}^K \Delta_{ik} \log \pi_{k}$$

---
exclude: true

## Soft K-means Clustering

Maximum likelihood estimates: 

$\hat{\mu}_k = \frac{\sum_{i=1}^N \Delta_{ik} x_i}{\sum_{i=1}^N\Delta_{ik}}$ 

$\hat{\sigma}_k^2 = \frac{\sum_{i=1}^N \Delta_{ik} (x_i-\hat{\mu}_k)^2}{\sum_{i=1}^N \Delta_{ik}}$

$\hat{\pi}_k=\frac{\sum_{i=1}^K}{N}$.

---
exclude: true

## Soft K-means Clustering

Of course, this result depends on observing values for $\Delta_{ik}$ which we don't observe. Use an iterative approach as well: 

- given current estimate of parameters $\theta$,  
- maximize $$E(\ell_0(\theta';X,\Delta)|X, \theta)$$. 


We can prove that maximizing this quantity also maximizes the likelihood we need $\ell(\theta;X)$. 

---
exclude: true

## Soft K-means Clustering

In the mixture case, what is the function we would maximize? 

Define

$$\gamma_{ik}(\theta) = E(\Delta_{ik}|X_i,\theta) = Pr(\Delta_{ik}=1|X_i,\theta)$$

---
exclude: true

## Soft K-means Clustering


Use Bayes' Rule to write this in terms of the multivariate normal densities with respect to current estimates $\theta$:

\begin{aligned}
\gamma_{ik} & = \frac{Pr(X_i|\Delta_{ik}=1)Pr(\Delta_{ik}=1)}{Pr(X_i)} \\
{} & = \frac{f_k(x_i;\mu_k,\sigma_k^2)\pi_k}{\sum_{l=1}^K f_l(x_i; \mu_l,\sigma_l^2)\pi_l}
\end{aligned}

---
exclude: true

## Soft K-means Clustering

Quantity $\gamma_{ik}(\theta)$ is referred to as the _responsibility_ of cluster $k$ for observation $i$, according to current parameter estimate $\theta$. 

---
exclude: true

## Soft K-means Clustering

Then the expectation we are maximizing is given by

$$E(\ell_0(\theta'; X, \Delta)| X, \theta) = 
\sum_{i=1}^N \sum_{k=1}^K \gamma_{ik}(\theta) \log f_k(x_i; \theta') +
\sum_{i=1}^N \sum_{k=1}^K \gamma_{ik}(\theta) \log \pi'_k$$

---
exclude: true

## Soft K-means Clustering

We can now give a complete specification of the EM algorithm for mixture model clustering.

0. Take initial guesses for parameters $\theta$  
1. _Expectation Step_: Compute responsibilities $\gamma_{ik}(\theta)$
2. _Maximization Step_: Estimate new parameters based on responsibilities as below.
3. Iterate steps 1 and 2 until convergence

---
exclude: true

## Soft K-means Algorithm

Estimates in the Maximization step are given by

$$\hat{\mu}_k = \frac{\sum_{i=1}^N \gamma_{ik}(\theta) x_i}{\sum_{i=1}^N \gamma_{ik}}$$

$$\hat{\sigma}_k^2 = \frac{\sum{i=1}^N \gamma_{ik}(\theta)(x_i-\mu_k)}{\sum_{i=1}^N\gamma_{ik}(\theta)}$$

and 

$$\hat{\pi}_k = \frac{\sum_{i=1}^N\gamma_{ik}(\theta)}{N}$$

---
exclude: true

## Soft K-means Algorithm

The name "soft" K-means refers to the fact that parameter estimates for each cluster are obtained by weighted averages across all observations.

This is an instance of the *Expectation-Maximization Algorithm* which we will see again later this semester.


---
layout: false

## Summary

- Clustering algorithms used to partition data into groups of similar observations  
- K-means and K-mediods: iterative algorithms to minimize a partition objective function  
- Optimization solution depends on initialization: K-means++ improved initialization  
- BFR Algorithm: how to solve for massive datasets in "almost" one pass of algorithm  

