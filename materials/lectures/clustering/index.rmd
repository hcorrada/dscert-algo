---
title: "Unsupervised Learning: Clustering Analysis"
author: "Héctor Corrada Bravo"
company: "University of Maryland"
date: "`r Sys.Date()`"
css: ["custom.css"]
output: 
  xaringan::moon_reader:
    lib_dir: libs
    seal: false
    includes:
      after_body: "custom.html"
    nature:
      ratio: "16:9"
---
class: title-slide, center, middle
count: false

```{r cowplot_setup, echo=FALSE, message=FALSE}
library(cowplot)
```

.banner[![](img/epiviz.png)]

.title[Unsupervised Learning: Clustering Analysis]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
CMSC 643: `r Sys.Date()`
]

.logo[![](img/logo.png)]

---

## Unsupervised Learning

So far we have seen "Supervised Methods" where our goal is to analyze a _response_ (or outcome) based
on various _predictors_. 

In many cases, especially for Exploratory Data Analysis, we want methods to extract patterns on
variables without analyzing a specific _response_. 

Methods for the latter case are called "Unsupervised Methods". Examples are _Principal Component Analysis_ and _Clustering_.

---

## Unsupervised Learning

Interpretation of these methods is much more _subjective_ than in Supervised Learning. 

For example: 
if we want to know if a given _predictor_ is related to _response_, we can perform statistical inference using hypothesis testing. 

---

## Unsupervised Learning

If we want to know which predictors are useful for prediction: use cross-validation to do model selection. 

Finally, if we want to see how well we can predict a specific response, we can use cross-validation to report on test error. 

---

## Unsupervised Learning

In unsupervised methods, there is no similar clean evaluation methodology. 

Nonetheless, they can be very useful methods to understand data at hand.

---

## Motivating Example

Time series dataset of mortgage affordability
as calculated and distributed by Zillow: https://www.zillow.com/research/data/. 

```{r clustering_setup, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(readr)
library(lubridate)

datadir <- "data"
url <- "http://files.zillowstatic.com/research/public/Affordability_Wide_2017Q1_Public.csv"
filename <- basename(url)
datafile <- file.path(datadir, filename)

if (!file.exists(datafile)) {
  download.file(url, file.path(datadir, filename))
}

afford_data <- read_csv(datafile)
```

```{r tidy_zillow, echo=FALSE, cache=TRUE}
tidy_afford <- afford_data %>%
  filter(Index == "Mortgage Affordability") %>% 
  drop_na() %>%
  filter(RegionID != 0) %>%
  dplyr::select(RegionID, matches("^[1|2]")) %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m")))

wide_afford_df <- tidy_afford %>%
  dplyr::select(RegionID, time, affordability) %>%
  spread(time, affordability)

value_mat <-  wide_afford_df %>%
  dplyr::select(-RegionID) %>%
  as.matrix() 
```

```{r zillow_stats, echo=FALSE}
ncounties <- nrow(wide_afford_df)
year_range <- range(year(tidy_afford$time))
```

The dataset consists of monthly mortgage affordability values for `r ncounties` counties with data from `r min(year_range)` to `r max(year_range)`. 

---

## Motivating Example

> "To calculate mortgage affordability, we first calculate the mortgage payment for the median-valued home in a metropolitan area by using the metro-level Zillow Home Value Index for a given quarter and the 30-year fixed mortgage interest rate during that time period, provided by the Freddie Mac Primary Mortgage Market Survey (based on a 20 percent down payment)."

---

## Motivating Example

> "Then, we consider what portion of the monthly median household income (U.S. Census) goes toward this monthly mortgage payment. Median household income is available with a lag. "

---
class: split-60

## Motivating Example

.column[
```{r zillow_plot1, echo=FALSE, cache=FALSE, warning=FALSE, fig.align="center"}
tidy_afford %>%
  ggplot(aes(x=time,y=affordability,group=factor(RegionID))) +
  geom_line(color="GRAY", alpha=3/4, size=1/2) +
  labs(title="County-Level Mortgage Affordability over Time",
          x="Date", y="Mortgage Affordability")
```
]

.column[Can we partition counties into groups of counties with similar value trends across time?]

---

## Preliminaries

In "Supervised Learning" we were concerned with estimates that minimize some error function relative to the outcome of interest $Y$:

$$\mu(x) = \arg \min_{\theta} E_{Y|X} L(Y, \theta)$$

---

## Preliminaries

In order to do this, explicitly or not, the methods we were using would be concerned with properties of the conditional probability distribution $Pr(Y|X)$, 

without concerning itself with probability distribution $Pr(X)$ of the covariates themselves. 

---

## Preliminaries

In unsupervised learning, we are interested in properties of $Pr(X)$. 

In our example, what can we say about the distribution of home value time series? 

Since the dimensionality of $Pr(X)$ can be large, unsupervised learning methods seek to find structured representations of $Pr(X)$ that would be possible to estimate. 

---

## Preliminaries

In _clustering_ we assume that predictor space is partitioned and that $Pr(X)$ is defined over those partitions. 

In _dimensionality reduction_ we assume that $Pr(X)$ is really defined over a space (manifold) of smaller dimension. We will start studying clustering first.

---

## Cluster Analysis

The high-level goal of cluster analysis is to organize objects (observations) that are _similar_ to each other into groups. 

We want objects within a group to be more _similar_ to each other than objects in different groups. 

--

Central to this high-level goal is how to measure the degree of _similarity_ between objects. 

A clustering method then uses the _similarity_ measure provided to it to group objects into clusters.

```{r setup_kmeans, echo=FALSE, warning=FALSE, message=FALSE}
library(broom)
library(stringr)
```

```{r kmeans1, cache=FALSE, echo=FALSE}
set.seed(1234)
kmeans_res <- kmeans(value_mat, centers=9)

augmented_data <- kmeans_res %>%
  broom::augment(wide_afford_df) %>%
  gather(time, affordability, matches("^X")) %>%
  mutate(time=stringr::str_replace(time, "X", "")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y.%m.%d"))) %>%
  rename(cluster=.cluster)
        
kmeans_centers <- kmeans_res %>%
  broom::tidy(col.names=colnames(value_mat)) %>%
  dplyr::select(cluster, matches("^[1|2]")) %>%
  gather(time, affordability, -cluster) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m-%d")))
```

---
class: split-50

## Cluster Analysis

.column[
```{r kmeans_plot, echo=FALSE, cache=TRUE, fig.height=6}
augmented_data %>%
  ggplot(aes(x=time, y=affordability)) +
    geom_line(aes(group=RegionID), color="GRAY", alpha=1/2, size=1/2) +
    facet_wrap(~cluster) +
    geom_line(data=kmeans_centers, color="BLACK", alpha=1/2, size=1/2) +
  labs(main="Kmeans Clustering (k=9)",
       xlab="Date", ylab="affordability") +
  theme(axis.text.x=element_text(angle=45, hjust=1))
```
]

.column[Result of the k-means algorithm partitioning the data into 9 clusters.

The darker series within each cluster shows the average time series within the cluster.
]

---

## Dissimilarity-based Clustering

For certain algorithms, instead of similarity we work with dissimilarity, often represented as distances. 

When we have observations defined over attributes, or predictors, we define dissimilarity based on these attributes. 

---

## Dissimilarity-based Clustering

Given measurements $x_{ij}$ for $i=1,\ldots,N$ observations over $j=1,\ldots,p$ predictors. 

Suppose we define a dissimilarity $d_j(x_{ij}, x_{i'j})$, we can then define a dissimilarity between objects as

$$d(x_i, x_{i'}) = \sum_{j=1}^p d_j(x_{ij},x_{i'j})$$

---

## Dissimilarity-based Clustering

In the k-means algorithm, and many other algorithms, the most common usage is squared distance

$$d_j(x_{ij},x_{i'j}) = (x_{ij}-x_{i'j})^2$$

We can use different dissimilarities, for example

$$d_j(x_{ij}, x_{i'j}) = |x_{ij}-x_{i'j}|$$

which may affect our choice of clustering algorithm later on. 

---

## Dissimilarity-based Clustering

For categorical variables, we could set 

$$d_j(x_{ij},x_{i'j}) = \begin{cases}
0 \; \textrm{if } x_{ij} = x_{i'j} \\\\
1 \; \textrm{o.w.}
\end{cases}$$


---

## Dissimilarity-based Clustering

If the values the categorical variable have an intrinsic similarity

Generalize using symmetric matrix $L$ with elements 

$L_{rr'} = L_{r'r}$,  
$L_{rr}=0$ and   
$L_{rr'} \geq 0$ otherwise. 

This may of course lead to a dissimilarity that is not a proper distance.

---


## K-means Clustering

A commonly used algorithm to perform clustering is the K-means algorithm. 

It is appropriate when using squared Euclidean distance as the measure of object dissimilarity.

$$\begin{aligned} d(x_{i},x_{i'}) & = \sum_{j=1}^p (x_{ij}-x_{i'j})^2 \\\\
{} & = \|x_i - x_{i'}\|^2
\end{aligned}$$

---

## K-means Clustering

K-means partitions observations into $K$ clusters, with $K$ provided as a parameter. 

Given some clustering, or partition, $C$, denote cluster assignment of observation $x_i$ to cluster $k \in \{1,\ldots,K\}$ is denoted as $C(i)=k$. 

--

K-means minimizes this clustering criterion:

$$W(C) = \frac{1}{2} \sum_{k=1}^K \sum_{i: \, C(i)=k} \sum_{i':\, C(i')=k} \|x_i - x_{i'}\|^2$$

---

## K-means Clustering

This is equivalent to minimizing

$$W(C) = \frac{1}{2}\sum_{k=1}^K N_k \sum_{i:\,C(i)=k} \|x_i - \bar{x}_k\|^2$$

with:

- $\bar{x}_k=(\bar{x}_{k1},\ldots,\bar{x}_{kp})$   
- $\bar{x}_{kj}$ is the average of predictor $j$ over the observations assigned to cluster $k$, 
- $N_k$ is the number of observations assigned to cluster $k$

---

## K-means Clustering

$$W(C) = \frac{1}{2}\sum_{k=1}^K N_k \sum_{i:\,C(i)=k} \|x_i - \bar{x}_k\|^2$$

Minimize the total distance given by each observation to the mean (centroid) of the cluster to which the observation is assigned.

---

## K-means Clustering

An iterative algorithm is used to minimize this criterion

0. Initialize by choosing $K$ observations as centroids $m_1,m_2,\ldots,m_k$
1. Assign each observation $i$ to the cluster with the nearest centroid, i.e., set $C(i)=\arg\min_{1 \leq k \leq K} \|x_i - m_k\|^2$
2. Update centroids $m_k=\bar{x}_k$
3. Iterate steps 1 and 2 until convergence

---
class: split-30

## K-means Clustering

.column[
Here we illustrate the k-means algorithm over four iterations on our example data with $K=4$. 
]

```{r kmeans_illustration, echo=FALSE, message=FALSE, cache=TRUE}
set.seed(1234)

K <- 4
nobs <- nrow(value_mat)
centroid_indices <- sample(nobs, K)
centroids <- value_mat[centroid_indices,]

assign_cluster <- function(x, m) {
#  browser()
  xx <- rowSums(x^2)
  mm <- rowSums(m^2)
  xm <- tcrossprod(x, m)
  
  d <- xx - 2*xm
  d <- sweep(d, 2, mm, FUN="+")
  apply(d, 1, which.min)
}

get_centroids <- function(x, a) {
  inds <- split(seq_len(nobs), a)
  sapply(inds, function(i) colMeans(x[i,,drop=FALSE])) %>%
    t()
}

message("Iteration 1")
assignments <- assign_cluster(value_mat, centroids)

centroid_df <- as.data.frame(cbind(centroids, cluster=seq_len(K), iteration=1))

assignments_df <- cbind(wide_afford_df, cluster=assignments, iteration=1)

for (it in seq(2,4)) {
  message("Iteration ", it)
  centroids <- get_centroids(value_mat, assignments)
  assignments <- assign_cluster(value_mat, centroids)
  
  tmp <- as.data.frame(cbind(centroids, cluster=seq_len(K), iteration=it))
centroid_df <- rbind(centroid_df, tmp)

  tmp <- cbind(wide_afford_df, cluster=assignments, iteration=it)
assignments_df <- rbind(assignments_df, tmp)
}
```

.column[
```{r kmeans_illustration2, echo=FALSE, cache=TRUE}
tall_assignments_df <- assignments_df %>%
  gather(time, affordability, matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m-%d"))) 

tall_centroids_df <- centroid_df %>% 
  gather(time,affordability,matches("^[1|2]")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m-%d")))

pl <- tall_assignments_df %>%
  ggplot(aes(x=time, y=affordability)) +
    geom_line(aes(group=RegionID), color="GRAY", alpha=1/2, size=1/2) + 
    facet_grid(iteration~cluster) +
    geom_line(data=tall_centroids_df, color="BLACK") +
    labs(xlab="Date", ylab="mortgage affordability") +
    theme(axis.text.x=element_text(angle=45, hjust=1))
show(pl)
```
]
---

## K-means Clustering

Criterion $W(C)$ is reduced in each iteration so the algorithm is assured to converge. 

Not a convex criterion, the clustering we obtain may not be globally optimal. 

In practice, the algorithm is run with multiple initializations (step 0) and the best clustering achieved is used. 

---

## K-means Clustering

Also, selection of observations as centroids can be improved using the K-means++ algorithm:

0. Choose an observation as centroid $m_1$ uniformly at random  
1. To choose centroid $m_k$, compute for each observation $i$ not chosen as a centroid the distance to the nearest centroid $d_i = \min_{1\leq l < k} \|x_i - m_l\|^2$
2. Set centroid $m_k$ to an observation randomly chosen with probability $\frac{e^d_i}{\sum_{i'} e^d_{i'}}$
3. Iterate steps 1 and 2 until $K$ centroids are chosen

---

## Choosing the number of clusters

The number of parameters must be determined before running the K-means algorithm. 

There is no clean direct method for choosing the number of clusters to use in the K-means algorithm (e.g. no cross-validation method)

```{r gapstat, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, results="hide"}
set.seed(1234)
gap_stat <- cluster::clusGap(value_mat, FUN=kmeans, nstart=15, B=100, K.max=9)

gap_stat_df <- gap_stat$Tab %>%
  as_tibble() %>%
  rowid_to_column("k")
```

---
class: split-30

## Choosing the number of clusters

.column[
Looking at criterion $W(C)$ alone is not sufficient as the criterion will become smaller as the value of $K$ is reduced. 
]

.column[
```{r logw_plot, echo=FALSE}
gap_stat_df %>%
  ggplot(aes(x=k, y=logW)) +
    geom_line() +
    geom_point()
```
]

---

## Choosing the number of clusters

We can use properties of this plot for ad-hoc selection.

Suppose there is a true underlying number $K^*$ of clusters in the data, 

- improvement in the $W_K(C)$ statistic will be fast for values of $K \leq K^*$
- slower for values of $K > K^*$. 

---

## Choosing the number of clusters

_Improvement in the $W_K(C)$ statistic will be fast for values of $K \leq K^*$_

In this case, there will be a cluster which will contain observations belonging to two of the true underlying clusters, and therefore will have poor within cluster similarity. 

As $K$ is increased, observations may then be separated into separate clusters, providing a sharp improvement in the $W_K(C)$ statistic. 

---

## Choosing the number of clusters

_Improvement in the $W_K(C)$ statistic will be slower for values of $K > K^*$_

In this case, observations belonging to a single true cluster are split into multiple cluster, all with generally high within-cluster similarity, 

Splitting these clusters further will not improve the $W_K(C)$ statistic very sharply. 

---
class: split-30

## Choosing the number of clusters

.column[The curve will therefore have an inflection point around $K^*$.
]

.column[
```{r logw_plot_again, echo=FALSE}
gap_stat_df %>%
  ggplot(aes(x=k, y=logW)) +
    geom_line() +
    geom_point()
```
]

---

## Choosing the number of clusters

The _gap statistic_ is used to identify the inflection point in the curve. 

It compares the behavior of the $W_K(C)$ statistic based on the data with the behavior of the $W_K(C)$ statistic for data generated uniformly at random over the range of the data. 

Chooses the $K$ that maximizes the gap between these two $W_K(C)$ curves.

---
class: split-30

## Choosing the number of clusters

.column[For this dataset, the gap statistic suggests there is no clear cluster structure and therefore $K=1$ is the best choice.

A choice of $K=4$ is also appropriate.]

.column[
```{r gapstat_plot, echo=FALSE}
factoextra::fviz_gap_stat(gap_stat)
```
]

---

## Soft K-means Clustering

Instead of the combinatorial approach of the $K$-means algorithm, take a more direct probabilistic approach to modeling distribution $Pr(X)$. 

Assume each of the $K$ clusters corresponds to a multivariate distribution $Pr_k(X)$, 

$Pr(X)$ is given by _mixture_ of these distributions as $Pr(X)=\sum_{k=1}^K \pi_k Pr_k(X)$. 

---

## Soft K-means Clustering

Specifically, take $Pr_k(X)$ as a multivariate normal distribution $f_k(X) = N(\mu_k, \sigma_k^2 I)$ 

and mixture density 
$f(X) = \sum_{k=1}^K \pi_k f_k(X)$. 

---

## Soft K-means Clustering

Use Maximum Likelihood to estimate parameters
$$\theta=(\mu_1, \ldots,\mu_K,\sigma_1^2,\ldots,\sigma_K^2,\pi_1,\ldots,\pi_K)$$ 

based on their log-likelihood
$$\ell(\theta;X) = \sum_{i=1}^N \log \left[ \sum_{k=1}^K \pi_k f_k(x_i;\theta) \right]$$

---

## Soft K-means Clustering

$$\ell(\theta;X) = \sum_{i=1}^N \log \left[ \sum_{k=1}^K \pi_k f_k(x_i;\theta) \right]$$

Maximizing this likelihood directly is computationally difficult

Use Expectation Maximization algorithm (EM) instead. 

---

## Soft K-means Clustering

Consider unobserved latent variables $\Delta_{ik}$ taking values 0 or 1, 

$\Delta_{ij}=1$ specifies observation $x_i$ was generated by component $k$ of the mixture distribution. 
---

## Soft K-means Clustering

Now set $Pr(\Delta_{ik}=1)=\pi_{k}$,and assume we _observed_ values for indicator variables $\Delta_{ik}$. 

We can write the log-likelihood of our parameters in this case as

$$\ell_0(\theta; X, \Delta) = \sum_{i=1}^N \sum_{k=1}^K \Delta_{ik} \log f_k(x_i; \theta) + \sum_{i=1}^N \sum_{k=1}^K \Delta_{ik} \log \pi_{k}$$

---

## Soft K-means Clustering

Maximum likelihood estimates: 

$\hat{\mu}_k = \frac{\sum_{i=1}^N \Delta_{ik} x_i}{\sum_{i=1}^N\Delta_{ik}}$ 

$\hat{\sigma}_k^2 = \frac{\sum_{i=1}^N \Delta_{ik} (x_i-\hat{\mu}_k)^2}{\sum_{i=1}^N \Delta_{ik}}$

$\hat{\pi}_k=\frac{\sum_{i=1}^K}{N}$.

---

## Soft K-means Clustering

Of course, this result depends on observing values for $\Delta_{ik}$ which we don't observe. Use an iterative approach as well: 

- given current estimate of parameters $\theta$,  
- maximize $$E(\ell_0(\theta';X,\Delta)|X, \theta)$$. 

--

We can prove that maximizing this quantity also maximizes the likelihood we need $\ell(\theta;X)$. 

---

## Soft K-means Clustering

In the mixture case, what is the function we would maximize? 

Define

$$\gamma_{ik}(\theta) = E(\Delta_{ik}|X_i,\theta) = Pr(\Delta_{ik}=1|X_i,\theta)$$

---

## Soft K-means Clustering


Use Bayes' Rule to write this in terms of the multivariate normal densities with respect to current estimates $\theta$:

\begin{aligned}
\gamma_{ik} & = \frac{Pr(X_i|\Delta_{ik}=1)Pr(\Delta_{ik}=1)}{Pr(X_i)} \\
{} & = \frac{f_k(x_i;\mu_k,\sigma_k^2)\pi_k}{\sum_{l=1}^K f_l(x_i; \mu_l,\sigma_l^2)\pi_l}
\end{aligned}

---

## Soft K-means Clustering

Quantity $\gamma_{ik}(\theta)$ is referred to as the _responsibility_ of cluster $k$ for observation $i$, according to current parameter estimate $\theta$. 

---

## Soft K-means Clustering

Then the expectation we are maximizing is given by

$$E(\ell_0(\theta'; X, \Delta)| X, \theta) = 
\sum_{i=1}^N \sum_{k=1}^K \gamma_{ik}(\theta) \log f_k(x_i; \theta') +
\sum_{i=1}^N \sum_{k=1}^K \gamma_{ik}(\theta) \log \pi'_k$$

---

## Soft K-means Clustering

We can now give a complete specification of the EM algorithm for mixture model clustering.

0. Take initial guesses for parameters $\theta$  
1. _Expectation Step_: Compute responsibilities $\gamma_{ik}(\theta)$
2. _Maximization Step_: Estimate new parameters based on responsibilities as below.
3. Iterate steps 1 and 2 until convergence

---

## Soft K-means Algorithm

Estimates in the Maximization step are given by

$$\hat{\mu}_k = \frac{\sum_{i=1}^N \gamma_{ik}(\theta) x_i}{\sum_{i=1}^N \gamma_{ik}}$$

$$\hat{\sigma}_k^2 = \frac{\sum{i=1}^N \gamma_{ik}(\theta)(x_i-\mu_k)}{\sum_{i=1}^N\gamma_{ik}(\theta)}$$

and 

$$\hat{\pi}_k = \frac{\sum_{i=1}^N\gamma_{ik}(\theta)}{N}$$

---

## Soft K-means Algorithm

The name "soft" K-means refers to the fact that parameter estimates for each cluster are obtained by weighted averages across all observations.

---
exclude: true

## General Model-based clustering

Clustering by mixtures can be generalized in many directions. 

For instance, we can expand the multivariate normal model used in each cluster such that
$f_k(X) \approx N(\mu_k,\Sigma_k)$ where $\Sigma_k$ is covariance matrix. 

---
exclude: true

## General Model-based Clustering

Since estimates based on likelihood, use criteria applicable to likelihood methods for model selection, e.g., Bayesian Information Criterion (BIC). 

This permits selection of the number of mixture components, and covariance parameterization. 

---
class: split-60
exclude: true

## General Model-based Clustering

.column[For our example data, we obtain a more reasonable result where the optimal number of clusters $K=8$ rather than $K=1$ obtained with the standard K-means algorithm.]

```{r load_mclust, echo=FALSE, message=FALSE, results="hide"}
library(mclust)
library(broom)
```

```{r mclust, echo=FALSE, message=FALSE, cache=TRUE, results="hide"}
mc_res <- mclust::Mclust(value_mat)
```

.column[
```{r mclust_bic, echo=FALSE, message=FALSE, fig.height=6}
plot(mc_res, what="BIC")
```
]

---
exclude: true

## General Model-based Clustering

```{r mclust_broom, echo=FALSE, message=FALSE}
mc_augmented_df <- mc_res %>%
  broom::augment(wide_afford_df) %>%
  dplyr::select(RegionID, cluster=.class, matches("^X[1|2]")) %>%
  gather(time, affordability, matches("^X[1|2]")) %>%
  mutate(time = stringr::str_replace(time, "^X","")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y.%m.%d")))

mc_centers <- mc_res %>%
  broom::tidy() %>%
  dplyr::select(cluster=component, starts_with("mean")) %>%
  gather(time, affordability, matches("^mean")) %>%
  mutate(time = stringr::str_replace(time, "mean.", "")) %>%
  type_convert(col_types=cols(time=col_date(format="%Y-%m-%d")))
```

```{r mclust_clusters, echo=FALSE, message=FALSE, fig.align="center"}
mc_augmented_df %>%
  ggplot(aes(x=time, y=affordability)) +
    geom_line(aes(group=RegionID), color="GRAY", alpha=1/2, size=1/2) +
    geom_line(data=mc_centers, color="BLACK", alpha=1/2, size=1/2) +
    facet_wrap(~cluster)
```

---
exclude: true

## General Model-based Clustering

Applying other cluster conditional distributions, the cluster mixture model can also be used for other datatypes where the normal distribution is not appropriate. 

There are also kernel-based methods, where a kernel function is used to define a dissimilarity measure, that apply clustering algorithms to situations where non-linearity via kernel methods is applicable.

---

## Summary 

Clustering methods are intuitive methods useful to understand structure within unlabeled observations. 

Model-based methods, using the EM algorithm, provide a large amount of flexibililty over simpler methods like the K-means algorithm.

Kernel-based clustering methods (Kernel K-means) permit the clustering of observations based on non-linear similarity functions.