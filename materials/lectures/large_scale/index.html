<!DOCTYPE html>
<html>
  <head>
    <title>Large Scale Learning</title>
    <meta charset="utf-8">
    <meta name="author" content="Héctor Corrada Bravo" />
    <meta name="date" content="2018-11-27" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Large Scale Learning]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
CMSC 643: 2018-11-27
]

.logo[![](img/logo.png)]

---
layout: true

## Large-scale Learning

---

Analyses we have done in class are for in-memory data: 

  - Datasets can be loaded onto memory of a single computing node. 
  
Database systems can execute SQL queries, which can be used for efficient learning of some models (e.g. decision trees) over data on disk 

  - Operations are usually performed by a single computing node. 

---

In the 90s database systems that operate over multiple computing nodes became available 

Basis of the first generation of large data warehousing. 

In the last decade, systems that manipulate data over multiple nodes have become standard. 

---

**Basic observation**

for very large datasets, many of the operations for aggregation and summarization, which also form the basis of many learning methods, can be parallelized. 

---


For example:

- partition observations and perform transformation on each partition as a parallel process
- partition variables and perform transformation on each variable as a parallel process
- for summarization (`group_by` and `summarize`), partition observations based on `group_by` expression, perform `summarize` on each partition.

---

Efficiency of implementation of this type of parallelism depends on underlying architecture: 

_Shared memory vs. Shared storage vs. Shared nothing_

For massive datasets, _shared nothing_ is usually preferred since fault tolerance is perhaps the most important consideration.

---
layout: true

## Map-reduce

---

Map-Reduce is an implementation idea for a shared nothing architecture. 
It is based on: 

- _distributed storage_ 
- _data proximity_ (perform operaations on data that is physically close)
- _fault tolerance_. 

---

Basic computation paradigm is based on two operations:

  - reduce: perform operation on subset of observations in parallel  
  - map: decide which parallel process (node) should operate on each observation
  
---

The fundamental operations that we have learned very well in this class are nicely represented in this framework: `group_by` clause corresponds to `map`, and  `summarize` function corresponds to `reduce`.

.center[![](images/mr1.png)]


---

Map-reduce is most efficient when computations are organized in an acyclic graph.

Data is moved from stable storage to computing process and the result moved to stable storage without much concern for operation ordering.

This architecture provides runtime benefits due to flexible resource allocation
and strong failure recovery. 

Existing implementations of Map-reduce systems do not support interactive use, or workflows that are hard to represent as acyclic graphs.

---
layout: true

## Spark

---

Recent system based on the general map-reduce framework

Designed for ultra-fast data analysis. 

Provides efficient support for interactive analysis (the kind we do in Jupyter) 

Designed to support iterative workflows needed by many Machine Learning algorithms.
  
---

The basic data abstraction in Spark is the resilient distributed dataset (RDD). 

Applications keep working sets of data in memory and support iterative algorithms and interactive workflows.

---

RDDs are

(1) inmutable and *partitioned* collections of objects,  
(2) created by parallel *transformations* on data in stable storage (e.g., map, filter, group_by, join, ...)  
(3) *cached* for efficient reuse  
(4) operated upon by actions defeind on RDDs (count, reduce, collect, save, ...)

---
exclude: true
class: split-30

### Example

.column[Let's use `SparkR` to illustrate how a Spark workflow is organized.
Let's imagine we want to count occurences of phrases in abstracts (based on regular expression searches) over a large scientific corpus.
]


.column[

```r
library(SparkR)
library(stringr)

# initialize spark framework
sc &lt;- sparkR.init("local")

# base RDD
lines = textFile(sc, "hdfs://...")

# transformed RDD
abstracts = filterRDD(lines, function(line) str_detect(unlist(line), "ABSTRACT"))

# mapped RDD
abstract_text = map(abstracts, function(line) str_split(unlist(line), " ")[[1]])

# cached RDD
cached_text = cache(abstract_text)

library(magrittr)

# RDD action
cached_text %&gt;% 
  filterRDD(function(line) str_detect(unlist(line), "foo")) %&gt;% 
  count
```
]

---
exclude: true

![](index_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

---

#### Fault Tolerance

RDDs maintain *lineage*, so partitions can be reconstructed upon failure.



---

### The components of a SPARK workflow

**Transformations**: Define new RDDs

[https://spark.apache.org/docs/latest/programming-guide.html#transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations)

**Actions**: Return results to driver program

[https://spark.apache.org/docs/latest/programming-guide.html#actions](https://spark.apache.org/docs/latest/programming-guide.html#actions)

---

Spark was designed first for Java with an interactive shell based on Scala. 
It has strong support in Python and increasing support in R SparkR.

- Spark programming guide: [https://spark.apache.org/docs/latest/programming-guide.html](https://spark.apache.org/docs/latest/programming-guide.html)
- More info on python API:
https://spark.apache.org/docs/0.9.1/python-programming-guide.html


---
layout: true

## Stochastic Gradient Descent

---

Other learning methods we have seen, like regression and SVMs (or even PCA), are **optimization problems** 

We can design gradient-descent based optimization algorithms that process  data efficiently.

We will use linear regression as a case study of how this insight would work.

---
layout: true

### Case Study

---

Let's use linear regression with one predictor, no intercept as a case study.

**Given**: Training set `\(\{(x_1, y_1), \ldots, (x_n, y_n)\}\)`, with continuous response `\(y_i\)` and single predictor `\(x_i\)` for the `\(i\)`-th observation.

**Do**: Estimate parameter `\(w\)` in model `\(y=wx\)` to solve

`$$\min_{w} L(w) = \frac{1}{2} \sum_{i=1}^n (y_i - w x_i)^2$$`

---
class: split-50

.column[Suppose we want to fit this model to the following (simulated) data:]

.column[
![](index_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
]

---
class: split-50

.column[Our goal is then to find the value of `\(w\)` that minimizes mean squared error. This corresponds to finding one of these many possible lines.]

.column[
![](index_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]

---
class: split-50

.column[Each of which has a specific error for this dataset:]


.column[
![](index_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]

---

Insights:

1) As we saw before in class, loss is minimized when the derivative of the loss function is 0

2) and, the derivative of the loss (with respect to `\(w\)` ) at a given estimate `\(w\)` suggests new values of `\(w\)` with smaller loss!

---
class: split-40

.column[Let's take a look at the derivative:

`$$\frac{\partial}{\partial w} L(w) = \\
\frac{\partial}{\partial w} \frac{1}{2} \sum_{i=1}^n (y_i - w x_i)^2 \\
{} = \sum_{i=1}^n (y_i - w x_i) (-x_i)$$`


]


.column[
&lt;img src="index_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;
]

---
layout: true

## Gradient Descent

---

This is what motivates the Gradient Descent algorithm

1. Initialize `\(w=\mathrm{normal(0,1)}\)`
2. Repeat until convergence
  - Set `\(w = w + \eta \sum_{i=1}^n (y_i - f(x_i)) x_i\)`
  

---

The basic idea is to move the current estimate of `\(w\)` in the direction that minimizes loss the *fastest*. 

---
class: split-50

.column[Let's run GD and track what it does:]

.column[




![](index_files/figure-html/unnamed-chunk-11-.gif)&lt;!-- --&gt;
]

---

"Batch" gradient descent: take a step (update `\(w\)`) by calculating derivative with respect to _all_ `\(n\)` observations in our dataset. 


`$$w = w + \eta \sum_{i=1}^n (y_i - f(x_i, w)) x_i$$`

where `\(f(x_i) = w x_i\)`.

---

For multiple predictors (e.g., adding an intercept), this generalizes to the _gradient_ 


$$
\mathbf{w} = \mathbf{w} + \eta \sum_{i=1}^n (y_i - f(\mathbf{x}_i, \mathbf{w})) \mathbf{x}_i
$$

where `\(f(\mathbf{x}_i, \mathbf{w}) = w_0 + w_1 x_{i1} + \cdots + w_p x_{ip}\)`

---

Gradiest descent falls within a family of optimization methods called _first-order methods_ (first-order means they use derivatives only). These methods have properties amenable to use with very large datasets:

1. Inexpensive updates    
2. "Stochastic" version can converge with few sweeps of the data  
3. "Stochastic" version easily extended to streams  
4. Easily parallelizable  

Drawback: Can take many steps before converging

---
layout: true

## Stochastic Gradient Descent

---

**Key Idea**: Update parameters using update equation _one observation at a time_:

1. Initialize `\(\beta=\mathrm{normal}(0,\sqrt{p})\)`, `\(i=1\)`
2. Repeat until convergence
  - For `\(i=1\)` to `\(n\)`
    - Set `\(w = w + \eta (y_i - f(\mathbf{x}_i, \mathbf{w})) \mathbf{x}_i\)`

---
class: split-50




.column[Let's run this and see what it does:]

.column[



![](index_files/figure-html/unnamed-chunk-14-.gif)&lt;!-- --&gt;
]

---
Why does SGD make sense?

For many problems we are minimizing a cost function of the type

$$
\arg \min_f \frac{1}{n} \sum_i L(y_i,f_i) + \lambda R(f)
$$
Which in general has gradient

$$
\frac{1}{n} \sum_i \nabla_f L(y_i,f_i) + \lambda \nabla_f R(f)
$$

---

$$
\frac{1}{n} \sum_i \nabla_f L(y_i,f_i) + \lambda \nabla_f R(f)
$$

The first term looks like an empirical estimate (average) of the gradient at `\(f_i\)`

SGD then uses updates provided by a different _estimate_ of the gradient based on a single point.

- Cheaper
- Potentially unstable

---

In practice

- Mini-batches: use ~100 or so examples at a time to estimate gradients
- Shuffle data order every pass (epoch)

---


SGD easily adapts to _data streams_ where we receive observations one at a time and _assume_ they are not stored. 

This setting falls in the general category of _online_ learning.

_Online learning_ is extremely useful in settings with massive datasets

---

### Parallelizing gradient descent

Gradient descent algorithms are easily parallelizable:

- Split observations across computing units  
- For each step, compute partial sum for each partition (map), compute final update (reduce)  

`$$\mathbf{w} = \mathbf{w} + \eta * \sum_{\mathrm{partition}\; q} \sum_{i \in q} (y_i - f(\mathbf{x_i}, \mathbf{w})) \mathbf{x}_i$$`

---

This observation has resulted in their implementation if systems for large-scale learning:

1. [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki)
  - Implements general framework of (sparse) stochastic gradient descent for many optimization problems

--

2. [Spark MLlib](https://spark.apache.org/docs/1.2.1/mllib-guide.html)
  - Implements many learning algorithms using Spark framework we saw previously
  
---
layout: true

## Sparse Regularization

---

For many ML algorithms prediction time-efficiency is determined
by the number of predictors used in the model.

Reducing the number of predictors can yield huge gains in efficiency in deployment. 

The amount of memory used to make predictions is also typically
governed by the number of features. (Note: this is not true of kernel
methods like support vector machines, in which the dominant cost is
the number of support vectors.) 

---

The idea behind sparse models, and in particular, sparse
regularizers. 

A disadvantage of optimizing problems of the form
$$
\sum_i L(y_i,w) + \lambda \|w\|^2
$$
That they tend to never produce weights that are
exactly zero. 

---

Instead, minimize problems of the form

$$
\sum_i L(y_i, w) + \lambda \|w\|_1
$$

where `\(\|w\|_1 = \sum_j |w_j|\)`

---

This is a convex optimization problem.

Can use standard subgradient methods.

See CIML for further details.

---
layout: true

## Feature Hashing

---

For data sets with a large number of features/attributes an idea based on hashing can also help.

Suppose we are building a model over `\(P\)` features ( `\(P\)` very large). We use hashing to reduce the number of features to smaller number `\(p\)`.

For each observation `\(x \in \mathbb{R}^P\)` we transform it into observation `\(\tilde{x} \in \mathbb{R}^p\)`.

We will use hash function `\(h: P \to p\)`

---

Algorithm:

Initialize `\(\tilde{x} = \langle 0, 0, ..., 0 \rangle\)`  
For each `\(j \in 1,\ldots,P\)`
  - Hash `\(j\)` to position `\(k = h(j)\)`
  - Update `\(\tilde{x}_k \gets \tilde{x}_k + x_j\)`
  
Return `\(\tilde{x}\)`

We can think of this as a _feature mapping_

`$$\phi(x)_k = \sum_{j | j \in h^{-1}(k)} x_j$$`

---

To see how what this does, we can see what the inner product between observations in the smaller feature space is:

`$$\phi(x)'\phi(z) = \sum_k \left[ \sum_{j|j\in h^{-1}(k)} x_j \right] \left[ \sum_{j'|j' \in h^{-1}(k)} z_{j'}\right] \\
{} = \sum_k \sum_{j,j'| j,j' \in h^{-1}(k)}  x_j z_{j'}\\
{} = \sum_j \sum_{j' | j' \in h^{-1}(h(j))} x_j z_{j'} \\
{} = \sum_j x_j z_j + \sum_{j' \neq j | j' \in h^{-1}(h(j))} x_j z_{j'} = x'z + \cdots$$`

---

So, we get the inner product in the original large dimensions plus an extra quadratic term

`$$\phi(x)'\phi(z) = x'z + \sum_{j' \neq j | j' \in h^{-1}(h(j))} x_j z_{j'}$$`

- We might get lucky and get a useful interaction between two features
- Nonetheless, the size of this sum is very small due to property of hash functions: _expected value_ of product is `\(\approx 0\)`

---
layout: false

## Large-Scale Learning

- Database operations for out-of-memory datasets
- Parallelization on shared-nothing architectures (map reduce)
- Spark as MR framework also supporting iterative procedures (optimization)
- Stochastic gradient descent (many low cost steps, easy to parallelize)
- Sparse models (build models with few features)
- Feature Hashing (build models over smaller number of features, retain inner product)
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>
<script>
remark.macros['scale'] = function (percentage) {
  var url = this;
  return '<img src="' + url + '" style=width: ' + percentage + '"/>';
};
</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
