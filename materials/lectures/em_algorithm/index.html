<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>The EM Algorithm</title>
    <meta charset="utf-8" />
    <meta name="author" content="Héctor Corrada Bravo" />
    <meta name="date" content="2020-04-26" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class: title-slide, center, middle
count: false



.banner[![](img/epiviz.png)]

.title[Algorithms for Data Science: The EM Algorithm]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
DATA 606: 2020-04-26
]

.logo[![](img/logo.png)]

---
exclude: true

## Motivating Example

Time series dataset of mortgage affordability
as calculated and distributed by Zillow: https://www.zillow.com/research/data/. 







The dataset consists of monthly mortgage affordability values for 76 counties with data from 1979 to 2017. 

---
exclude: true

## Motivating Example

&gt; "To calculate mortgage affordability, we first calculate the mortgage payment for the median-valued home in a metropolitan area by using the metro-level Zillow Home Value Index for a given quarter and the 30-year fixed mortgage interest rate during that time period, provided by the Freddie Mac Primary Mortgage Market Survey (based on a 20 percent down payment)."

---
exclude: true

## Motivating Example

&gt; "Then, we consider what portion of the monthly median household income (U.S. Census) goes toward this monthly mortgage payment. Median household income is available with a lag. "

---
class: split-60
exclude: true

## Motivating Example

.column[
&lt;img src="index_files/figure-html/zillow_plot1-1.png" style="display: block; margin: auto;" /&gt;
]

.column[Can we partition counties into groups of counties with similar value trends across time?]

---
layout: false

## Soft K-means Clustering

Instead of the combinatorial approach of the `\(K\)`-means algorithm, take a more direct probabilistic approach to modeling distribution `\(P(X)\)`. 

Assume each of the `\(K\)` clusters corresponds to a multivariate distribution `\(P_k(X)\)`, 

`\(P(X)\)` is then a _mixture_ of these distributions as `\(P(X)=\sum_{k=1}^K \pi_k P_k(X)\)`. 

---

## Soft K-means Clustering

Specifically, take `\(P_k(X)\)` as a multivariate normal distribution `\(f_k(X) = N(\mu_k, \sigma_k^2 I)\)` 

and mixture density 
`\(f(X) = \sum_{k=1}^K \pi_k f_k(X)\)`. 

---

## Soft K-means Clustering

Use Maximum Likelihood to estimate parameters
`$$\theta=(\mu_1, \ldots,\mu_K,\sigma_1^2,\ldots,\sigma_K^2,\pi_1,\ldots,\pi_K)$$` 

based on their log-likelihood
`$$\ell(\theta;X) = \sum_{i=1}^N \log \left[ \sum_{k=1}^K \pi_k f_k(x_i;\theta) \right]$$`

---

## Soft K-means Clustering

`$$\ell(\theta;X) = \sum_{i=1}^N \log \left[ \sum_{k=1}^K \pi_k f_k(x_i;\theta) \right]$$`

Maximizing this likelihood directly is computationally difficult

Use Expectation Maximization algorithm (EM) instead. 

---

## Example: Mixture of Two Univariate Gaussians

&lt;img src="index_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

## Soft K-means Clustering

Consider unobserved latent variables `\(\Delta_{ik}\)` taking values 0 or 1, 

`\(\Delta_{ij}=1\)` specifies observation `\(x_i\)` was generated by component `\(k\)` of the mixture distribution.

---

## Soft K-means Clustering

Now set `\(Pr(\Delta_{ik}=1)=\pi_{k}\)`,and assume we _observed_ values for latent variables `\(\Delta_{ik}\)`. 

We can write the log-likelihood in this case as

`$$\ell_0(\theta; X, \Delta) = \sum_{i=1}^N \sum_{k=1}^K \Delta_{ik} \log f_k(x_i; \theta) + \sum_{i=1}^N \sum_{k=1}^K \Delta_{ik} \log \pi_{k}$$`

---

## Soft K-means Clustering

We have closed-form solutions for maximum likelihood estimates: 

`\(\hat{\mu}_k = \frac{\sum_{i=1}^N \Delta_{ik} x_i}{\sum_{i=1}^N\Delta_{ik}}\)` 

`\(\hat{\sigma}_k^2 = \frac{\sum_{i=1}^N \Delta_{ik} (x_i-\hat{\mu}_k)^2}{\sum_{i=1}^N \Delta_{ik}}\)`

`\(\hat{\pi}_k=\frac{\sum_{i=1}^K \Delta_{ik}}{N}\)`.


---
layout: true

## Constrained optimization

---

We have a problem of type 

$$
`\begin{array}
{} \mathrm{min}_x &amp; f_0(x) \\
\textrm{s.t.} &amp; f_i(x) \leq 0 \; i=1,\ldots,m \\
{} &amp; h_i(x) = 0\; i=1,\ldots,p
\end{array}`
$$

Note: This discussion follows Boyd and Vandenberghe, _Convex Optimization_

---

To solve these type of problems we will look at the _Lagrangian_ function:

`$$L(x,\lambda,\nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) + \sum_{i=1}^p \nu_i g_i(x)$$`

---

There is a beautiful result giving _optimality conditions_ based on the Lagrangian:

Suppose `\(\tilde{x}\)`, `\(\tilde{\lambda}\)` and `\(\tilde{\nu}\)` are _optimal_, then

$$
`\begin{array}
{} f_i(\tilde{x}) \leq 0 \\
h_i(\tilde{x}) = 0 \\
\tilde{\lambda_i} \geq 0 \\
\tilde{\lambda_i}f_i(\tilde{x}) = 0 \\
\nabla L(\tilde{x},\tilde{\lambda},\tilde{\nu}) = 0
\end{array}`
$$
---

We can use the gradient and feasibility conditions to prove the MLE result.

---

## Soft K-means Clustering

Of course, this result depends on observing values for `\(\Delta_{ik}\)` which _we don't observe_. Use an iterative approach as well: 

- given current estimate of parameters `\(\theta\)`,  
- Substitute `\(E[\Delta_{ik}|X_i,\theta]\)` for `\(\Delta_{ik}\)`. 

--

We will prove that this maximizes the likelihood we need `\(\ell(\theta;X)\)`. 

---

## Soft K-means Clustering

In the mixture case, what does this look like? 

Define

`$$\gamma_{ik}(\theta) = E(\Delta_{ik}|X_i,\theta) = Pr(\Delta_{ik}=1|X_i,\theta)$$`

---

## Soft K-means Clustering


Use Bayes' Rule to write this in terms of the multivariate normal densities with respect to current estimates `\(\theta\)`:

`\begin{aligned}
\gamma_{ik} &amp; = \frac{Pr(X_i|\Delta_{ik}=1)Pr(\Delta_{ik}=1)}{Pr(X_i)} \\
{} &amp; = \frac{f_k(x_i;\mu_k,\sigma_k^2)\pi_k}{\sum_{l=1}^K f_l(x_i; \mu_l,\sigma_l^2)\pi_l}
\end{aligned}`

---

## Soft K-means Clustering

Quantity `\(\gamma_{ik}(\theta)\)` is referred to as the _responsibility_ of cluster `\(k\)` for observation `\(i\)`, according to current parameter estimate `\(\theta\)`. 

---
exclude: true

## Soft K-means Clustering

Then the expectation we are maximizing is given by

`$$E(\ell_0(\theta'; X, \Delta)| X, \theta) = 
\sum_{i=1}^N \sum_{k=1}^K \gamma_{ik}(\theta) \log f_k(x_i; \theta') +
\sum_{i=1}^N \sum_{k=1}^K \gamma_{ik}(\theta) \log \pi'_k$$`

---

## Soft K-means Clustering

We can now give a complete specification of the EM algorithm for mixture model clustering.

1.  Take initial guesses for parameters `\(\theta\)`  
2.  _Expectation Step_: Compute responsibilities `\(\gamma_{ik}(\theta)\)`
3.  _Maximization Step_: Estimate new parameters based on responsibilities as below.
4.  Iterate steps 2 and 3 until convergence

---

## Soft K-means Algorithm

Estimates in the Maximization step are given by

`$$\hat{\mu}_k = \frac{\sum_{i=1}^N \gamma_{ik}(\theta) x_i}{\sum_{i=1}^N \gamma_{ik}}$$`

`$$\hat{\sigma}_k^2 = \frac{\sum_{i=1}^N \gamma_{ik}(\theta)(x_i-\mu_k)^2}{\sum_{i=1}^N\gamma_{ik}(\theta)}$$`

and 

`$$\hat{\pi}_k = \frac{\sum_{i=1}^N\gamma_{ik}(\theta)}{N}$$`

---

## Soft K-means Algorithm

The name "soft" K-means refers to the fact that parameter estimates for each cluster are obtained by weighted averages across all observations.

---
layout: true
## The EM Algorithm in General

---

So, why does that work?

Why does plugging in `\(\gamma_{ik}(\theta)\)` for the latent variables `\(\Delta_{ik}\)` work?

Why does that maximize log-likelihood `\(\ell(\theta;X)\)`?

---

Think of it as follows:

`\(Z\)`: observed data  
`\(Z^m\)`: missing _latent_ data
`\(T=(Z,Z^m)\)`: complete data (observed and missing)

--

`\(\ell(\theta';Z)\)`: log-likehood w.r.t.  *observed* data  
`\(\ell_0(\theta';T)\)`: log-likelihood w.r.t. *complete* data

---

Next, notice that

$$
Pr(Z|\theta') = \frac{Pr(T|\theta')}{Pr(Z^m|Z,\theta')}
$$

--

As likelihood:

$$
\ell(\theta';Z) = \ell_0(\theta';T) - \ell_1(\theta';Z^m|Z)
$$

---

Iterative approach: given parameters `\(\theta\)` take expectation of log-likelihoods

$$
`\begin{array}
\, \ell(\theta';Z) &amp; = &amp; E[\ell_0(\theta';T)|Z,\theta] - E[\ell_1(\theta';Z^m|Z)|Z,\theta] \\
{} &amp; \equiv &amp; Q(\theta',\theta) - R(\theta',\theta)
\end{array}`
$$

--

In soft k-means, `\(Q(\theta',\theta)\)` is the log likelihood of complete data with `\(\Delta_{ik}\)` replaced by `\(\gamma_{ik}(\theta)\)`

---

The general EM algorithm

1. Initialize parameters `\(\theta^{(0)}\)`  
2. Construct _function_ `\(Q(\theta',\theta^{(j)})\)`  
3. Find next set of parameters `\(\theta^{(j+1)} = \arg \max_{\theta'} Q(\theta', \theta^{(j)})\)`  
4. Iterate steps 2 and 3 until convergence  

---

So, why does that work?

$$
`\begin{aligned}
\ell(\theta^{(j+1)};Z) - \ell(\theta^{(j)};Z) &amp; = &amp;
[Q(\theta^{(j+1)},\theta^{(j)}) - Q(\theta^{(j)},\theta^{(j)})] \\ 
{} &amp; {} &amp; - [R(\theta^{(j+1)},\theta^{(j)})-R(\theta^{(j)},\theta^{(j)})] \\
{} &amp; \geq &amp; 0
\end{aligned}`
$$

--

I.E., every step makes log-likehood larger

---

Why else does it work? `\(Q(\theta',\theta)\)` _minorizes_ `\(\ell(\theta';Z)\)`

&lt;img src="img/minorizes.png" width="2355" /&gt;

---

General algorithmic concept:

Iterative approach:
  - Initialize parameters
  - Construct bound based on current parameters
  - Optimize bound
  
  
---
layout: false

## Imputing missing data

`\(Z\)`: observed data  
`\(Z^m\)`: missing observations

Requires a likelihood model...

---
layout: true

## Latent semantic analysis

---

Documents as _mixtures_ of topics (Hoffman 1998) 

&lt;img src="img/plsa.png" width="1784" /&gt;

---

Notation:

We have a set of documents `\(D\)`

Each document modeled as a bag-of-words (bow) over dictionary `\(W\)`.

`\(x_{w,d}\)`: the number of times word `\(w \in W\)` appears in document `\(d \in D\)`.

---

Let's start with a simple model based on the frequency of word occurrences.

Each document is modeled as `\(n_d\)` draws from a _Multinomial_ distribution with parameters `\(\theta_d = \{ \theta_{1,d}, \ldots, \theta_{W,d} \}\)`

Note `\(\theta_{w,d} \geq 0\)` and `\(\sum_{w} \theta_{w,d} = 1\)`.

---

_Probability of observed corpus `\(D\)`_

`$$Pr(D|\{ \theta_d \} ) \propto \prod_{d=1}^D \prod_{w=1}^W \theta_{w,d}^{x_{w,d}}$$`


---

## Problem 1:

Prove MLE `\(\hat{\theta}_{w,d} = \frac{x_{w,d}}{n_d}\)`
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>
<script>
remark.macros['scale'] = function (percentage) {
  var url = this;
  return '<img src="' + url + '" style=width: ' + percentage + '"/>';
};
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
