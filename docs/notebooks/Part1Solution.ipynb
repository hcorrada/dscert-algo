{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA with EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to generate a simulated document-word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# set simulation parameters\n",
    "num_docs = 20\n",
    "num_words = 100\n",
    "num_topics = 3\n",
    "\n",
    "# here we fix the number of words per\n",
    "# document to a constant number\n",
    "# could use a randomly generated\n",
    "# number of each document\n",
    "num_words_per_doc = 20\n",
    "\n",
    "\n",
    "# generate a document-word matrix given model parameters\n",
    "# p: topic distribution for each document\n",
    "# theta: word distribution for each topic\n",
    "def generate_mat(num_docs, num_words, num_topics, num_words_per_doc, p, theta):\n",
    "    # this will store the document word matrix\n",
    "    mat = np.zeros((num_docs, num_words))\n",
    "    \n",
    "    # this will store word-topic-document counts (unobserved) \n",
    "    delta = np.zeros((num_docs,num_words,num_topics))\n",
    "    for d in range(num_docs):\n",
    "        # generate number of words for each topic\n",
    "        nwt = np.random.multinomial(num_words_per_doc, p[d,:])\n",
    "        for t, n in np.ndenumerate(nwt):\n",
    "            # generate number of occurences for each word\n",
    "            delta[d,:,t] = np.random.multinomial(n, theta[t[0],:])\n",
    "            \n",
    "            # sum word occurrences over topics\n",
    "            mat[d,:] += delta[d,:,t][0,:]\n",
    "    return mat, delta\n",
    "        \n",
    "# generate a dataset\n",
    "#    - generate model parameters from a Dirichlet distribution\n",
    "#    - generates document-word matrix with function 'generate_mat'\n",
    "def generate_data(num_docs, num_words, num_topics, num_words_per_doc):\n",
    "    # generate topic distribution for each document\n",
    "    p = np.zeros((num_docs, num_topics))\n",
    "    \n",
    "    # assign extra weight to a specific topic in each document\n",
    "    # by using a non-uniform alpha parameter to the Dirichlet\n",
    "    # distribution\n",
    "    for d in range(num_docs):\n",
    "        # select which topic will be important\n",
    "        alpha = 1. * np.ones(num_topics)\n",
    "        t = d % num_topics\n",
    "        alpha[t] = 10.\n",
    "        p[d,:] = np.random.dirichlet(alpha)\n",
    "        \n",
    "    # generate word distribution for each topic\n",
    "    theta = np.zeros((num_topics, num_words))\n",
    "    alpha = np.ones((num_topics, num_words))\n",
    "    \n",
    "    # set some number of useful words (with high probability)\n",
    "    # for each topic\n",
    "    n_useful_words = 5 * num_topics\n",
    "    \n",
    "    for w in range(n_useful_words):\n",
    "        t = w % num_topics\n",
    "        alpha[t,w] = 10.\n",
    "        \n",
    "    for t in range(num_topics):\n",
    "        theta[t,:] = np.random.dirichlet(alpha[t,:])\n",
    "        \n",
    "    mat,delta = generate_mat(num_docs, num_words, num_topics, num_words_per_doc, p, theta)\n",
    "    return mat, delta, p, theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat, delta,p, theta = generate_data(num_docs,num_words,num_topics,num_words_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
