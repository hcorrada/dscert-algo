<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Project 1: Similar document searching via MinHash and Locality Sensitive Hashing - CMSC643: Machine Learning and Data Mining</title>
    <meta name="generator" content="Hugo 0.24" />

    
    <meta name="description" content="Machine Learning and Data Mining course. Part of the Department of Computer Science of the University of Maryland&#39;s Graduate Certificate in Data Science">
    
    <link rel="canonical" href="http://www.hcbravo.org/dscert-mldm/projects/project_1/">
    
    <meta name="author" content="HÃ©ctor Corrada Bravo">
    

    <meta property="og:url" content="http://www.hcbravo.org/dscert-mldm/projects/project_1/">
    <meta property="og:title" content="CMSC643: Machine Learning and Data Mining">
    <meta property="og:image" content="http://www.hcbravo.org/dscert-mldm/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="CMSC643: Machine Learning and Data Mining">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="http://www.hcbravo.org/dscert-mldm/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="http://www.hcbravo.org/dscert-mldm/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('http://www.hcbravo.org/dscert-mldm/fonts/icon.eot');
        src: url('http://www.hcbravo.org/dscert-mldm/fonts/icon.eot')
               format('embedded-opentype'),
             url('http://www.hcbravo.org/dscert-mldm/fonts/icon.woff')
               format('woff'),
             url('http://www.hcbravo.org/dscert-mldm/fonts/icon.ttf')
               format('truetype'),
             url('http://www.hcbravo.org/dscert-mldm/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="http://www.hcbravo.org/dscert-mldm/stylesheets/application.css">
    <link rel="stylesheet" href="http://www.hcbravo.org/dscert-mldm/stylesheets/temporary.css">
    <link rel="stylesheet" href="http://www.hcbravo.org/dscert-mldm/stylesheets/palettes.css">
    <link rel="stylesheet" href="http://www.hcbravo.org/dscert-mldm/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,700|Roboto&#43;Mono">
    <style>
      body, input {
        font-family: 'Roboto', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Roboto Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="http://www.hcbravo.org/dscert-mldm/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-blue">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Project 1: Similar document searching via MinHash and Locality Sensitive Hashing
      </div>
    </div>

    
    <div class="button button-twitter" role="button" aria-label="Twitter">
       <a href="https://twitter.com/hcorrada" title="@hcorrada on Twitter" target="_blank" class="toggle-button icon icon-twitter"></a>
    </div>
    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/hcorrada" title="@hcorrada on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>

</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="http://www.hcbravo.org/dscert-mldm/" class="project">
    <div class="banner">
      
        <div class="logo">
          <img src="http://www.hcbravo.org/dscert-mldm/images/logo.png">
        </div>
      
      <div class="name">
        <strong>CMSC643: Machine Learning and Data Mining </strong>
        
          <br>
          hcorrada/dscert-mldm.git
        
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      
        <hr>
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Calendar" href="http://www.hcbravo.org/dscert-mldm/calendar/">
	
	Calendar
</a>



  
</li>



<li>
  
    



<a  title="Logistics" href="http://www.hcbravo.org/dscert-mldm/logistics/">
	
	Logistics
</a>



  
</li>



<li>
  
    



<a  title="Resources" href="http://www.hcbravo.org/dscert-mldm/resources/">
	
	Resources
</a>



  
</li>



<li>
  
    



<a  title="Projects" href="http://www.hcbravo.org/dscert-mldm/projects/">
	
	Projects
</a>



  
</li>


        </ul>
        

        
        <hr>
        <span class="section">The author</span>
        
        <ul>
          
          <li>
            <a href="https://twitter.com/hcorrada" target="_blank" title="@hcorrada on Twitter">
              @hcorrada on Twitter
            </a>
          </li>
          

          
          <li>
            <a href="https://github.com/hcorrada" target="_blank" title="@hcorrada on GitHub">
              @hcorrada on GitHub
            </a>
          </li>
          
        </ul>
        

        <hr>
        <a href="https://www.umd.edu/web-accessibility" target="_blank" title="UMD Web Accessibility">Web Accessibility</a>

      </div>
    </div>
  </div>
</nav>


	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Project 1: Similar document searching via MinHash and Locality Sensitive Hashing </h1>

			

<p><strong>Due: Multiple due dates, see below</strong><br />
<strong>Posted: Sept. 1, 2018</strong><br />
<strong>Last Update:  Sept. 19, 2018</strong></p>

<p>In this first project we will implement the system described in the lecture notes for similar document searching.
This project is inspired by <a href="http://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/">http://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/</a> (Note: you can look at code there
for inspiration but implement your own).</p>

<h2 id="the-task">The Task</h2>

<p>We will use documents from this repository <a href="http://www.inf.ed.ac.uk/teaching/courses/tts/assessed/assessment3.html">http://www.inf.ed.ac.uk/teaching/courses/tts/assessed/assessment3.html</a>.
This is a dataset of documents for which we want to find possible plagiarism. It consists of 10,000 documents for which
some pairs are tagged as instances of plagiarism. The goal of this exercise is to see how effectively and efficiently
a minhash and LSH system can identify these instances.</p>

<p>Note that smaller subsets of data suitable for testing are available here:
<a href="https://github.com/chrisjmccormick/MinHash/tree/master/data">https://github.com/chrisjmccormick/MinHash/tree/master/data</a></p>

<h1 id="part-i-preliminaries">Part I: Preliminaries</h1>

<p><strong>DUE: Monday Sept. 10</strong></p>

<h2 id="part-ia-dataset-parsing">Part IA: Dataset parsing</h2>

<p>Write a function <code>parse_data</code> that given the path to a filename, reads in the article data and returns an array of tuples. With</p>

<ul>
<li>One tuple per article (there is one article per line)<br /></li>
<li>For each article tuples will contain <code>(id, string)</code> where <code>id</code> is the article id and <code>string</code> is the article text as described next</li>
<li>Process the article text to

<ol>
<li>remove all punctuation</li>
<li>change all letters to lowercase</li>
<li>remove all whitespace so that all words are concatenated</li>
</ol></li>
</ul>

<p>The function should have skeleton:</p>

<pre><code class="language-python">def parse_data(filename):
  # read lines from filename
  # construct tuple of id and text
  # process string as described above
  # return tuple with id and processed string
</code></pre>

<h2 id="part-ib-document-shingles">Part IB: Document shingles</h2>

<p>Write a function <code>shingle_document</code> that given a processed article string and a parameter <code>k</code> shards the document as follows:</p>

<ul>
<li>each substring of length $k$ in document is hashed to a 32-bit integer (see <code>crc32</code> fucntion in <a href="https://docs.python.org/3/library/binascii.html">https://docs.python.org/3/library/binascii.html</a>)</li>
<li>returns a list of the unique 32-bit integers obtained in previous step (look at python <code>sets</code> for this)</li>
</ul>

<p>The function should have skeleton</p>

<pre><code class="language-python">def shingle_document(string, k):
  # initialize set data structure
  # for each position in string,
    # extract substring of length k
    # hash substring into 32-bit integer using crc32
    # insert into set
  # return set
</code></pre>

<h2 id="part-ic-jaccard-similarity">Part IC: Jaccard Similarity</h2>

<p>Write a function <code>jaccard</code> that given two sharded documents, computes their Jaccard distance</p>

<p>Function should have skeleton</p>

<pre><code class="language-python">def jaccard(a, b):
  # compute union size
  # compute intersection size
  # return ratio of union and intersection
</code></pre>

<h2 id="part-id-put-these-together">Part ID: Put these together</h2>

<p>Write a function that uses the above to do the following:</p>

<ul>
<li>Parse a file with data</li>
<li>Return a list of tuples each tuple contains: <code>(id1, id2, s)</code>, where <code>id1</code> and <code>id2</code> are document ids and <code>s</code> is the computed Jaccard similarity</li>
</ul>

<h2 id="part-ie-experiment-0">Part IE: Experiment 0</h2>

<p>Use your function to answer the following question:</p>

<p>What is the effect of sharding length `k** on the Jaccard similarity of plagiarism instances versus instances that are not plagiarized?</p>

<p>To answer this question, make a plot with $k$ in the x-axis and average Jaccard similarity in the y-axis. Plot two lines,
one line for plagiarism instances, one line for instances that are not plagiarized.
Use the 1000 document dataset for this.</p>

<h1 id="part-ii-minhash">Part II: MinHash</h1>

<p><strong>DUE: Tuesday Sept. 18</strong></p>

<p>In this section you will implement the MinHash algorithm and perform an experiment to see how well it estimates Jaccard similarity.</p>

<h2 id="part-iia-prepare-shingles-for-processing">Part IIA: Prepare shingles for processing</h2>

<p>Implement a function that takes the shingled documents and returns a list of item-document pairs sorted by items that we&rsquo;ll use to compute the minhash signature of each document. Remember that because of the shingling logic we used above, we represent items as 32-bit integers. The function specs are as follows:</p>

<ul>
<li>Input is a list of tuples of form <code>(docid, [items])</code></li>
<li>Output will be a tuple with two elements:

<ul>
<li>a list of tuples of form <code>(item, docid)</code>. It will contain one entry for each item appearing in each document.</li>
<li>a list of document ids found in the dataset</li>
</ul></li>
<li>Output should be sorted by <code>item</code></li>
</ul>

<p>This function should have skeleton</p>

<pre><code class="language-python">def invert_shingles(shingled_documents):
  # initialize list for tuples
  # initialize list for document ids
  # for each document in input
    # append document id to list
    
    # for each item in document
      # append (item, docid) tuple
  
  # sort tuple list
  # return sorted tuple list, and document list
</code></pre>

<h2 id="part-iib-generate-hash-functions">Part IIB: Generate hash functions</h2>

<p>Use the <code>generate_random_hash_fn</code> function below to create function <code>make_hashes</code>. Given input <code>num_hashes</code> this function will return a list
of hash functions that mimic the random permutation approach used in Minhash calculation. The function specs are:</p>

<ul>
<li>Input is an integer <code>num_hash</code></li>
<li>Output is a list of hash functions created by function <code>generate_random_hash_fn</code></li>
</ul>

<h2 id="part-iic-construct-the-minhash-signature-matrix">Part IIC: Construct the Minhash Signature Matrix</h2>

<p>Implement a function that builds the Minhash signature matrix. You can use this code as a starting point. It refers to the
functions you implemented above and sketches the construction algorithm.</p>

<pre><code class="language-python">import numpy

def make_minhash_signature(shingled_data, num_hashes):
  inv_index, docids = invert_shingles(shingled_data)
  num_docs = len(docids)
  
  # initialize the signature matrix with infinity in every entry
  sigmatrix = np.full([num_hash, num_docs], np.inf)
  
  # generate hash functions
  hash_funcs = make_hashes(num_hashes)
  
  # iterate over each non-zero entry of the characteristic matrix
  for row, docid in inv_index:
    # update signature matrix if needed 
    # THIS IS WHAT YOU NEED TO IMPLEMENT
  
  return sigmatrix, docids
</code></pre>

<h2 id="part-iid-minhash-similarity-estimate">Part IID: MinHash similarity estimate</h2>

<p>Write a function that computes the similarity of two documents using the minhash matrix computed above. The function specs are:</p>

<ul>
<li>Input:

<ul>
<li><code>id1</code>, <code>id2</code>: document ids</li>
<li><code>minhash_sigmat</code>: minhash signature matrix</li>
<li><code>docids</code>: list of document ids, used to index the columns of the minhash signature matrix</li>
</ul></li>
<li>Output: Jaccard similarity estimated using minhash</li>
</ul>

<p>Here is the function skeleton:</p>

<pre><code class="language-python">def minhash_similarity(id1, id2, minhash_sigmat, docids):
  # get column of the similarity matrix for the two documents
  # calculate the fraction of rows where two columns match
  # return this fraction as the minhash similarity estimate
</code></pre>

<p>See hint below on comparing numpy arrays</p>

<h2 id="part-iie-put-these-together">Part IIE: Put these together</h2>

<p>Write a function that given shingled documents computes the Minhash estimated similarities between each pair of documents. This will be similar
to your function for Part ID.</p>

<h2 id="part-iif-experiment-1">Part IIF: Experiment 1</h2>

<p>Use your function to carry out the following experiment:</p>

<p>What is the effect of the number of hash functions used to compute the Minhash signature on the accuracy of the
Minhash estimate of Jaccard similarity. Carry out this experiment on the 1000 document dataset.</p>

<h1 id="part-iii-locality-sensitive-hashing">Part III: Locality-Sensitive Hashing</h1>

<p><strong>DUE: Tuesday Sept. 25</strong></p>

<h2 id="implement-lsh">Implement LSH</h2>

<p>Write a function that implements locality sensitive hashing. Function specifications:</p>

<ul>
<li>Input:

<ul>
<li><code>minash_sigmatrix</code>: a minhash signature matrix</li>
<li><code>numhashes</code>: number of hash functions used to construct minhash signature matrix</li>
<li><code>docids</code>: list of document ids</li>
<li><code>threshold</code> a minimum Jaccard similarity threshold</li>
</ul></li>
<li>Output:

<ul>
<li>a list of hash tables
<br /></li>
</ul></li>
</ul>

<pre><code class="language-python">from collections import defaultdict

def do_lsh(minhash_sigmatrix, numhashes, docids, threshold):
  b, _ = choose_nbands(threshold, numhashes)
  r = int(numhashes / b)
  narticles = len(docids)
  hash_func = _make_vector_hash(r)
  
  buckets = []
  for band in range(b):
    start_index = int(band * r)
    end_index = min(start_index + r, numhashes)
    
    cur_buckets = defaultdict(list)
    
    for j in range(narticles):
      # THIS IS WHAT YOU NEED TO IMPLEMENT
    
    buckets.append(cur_buckets)
  
  return buckets
</code></pre>

<h2 id="find-candidate-similar-article-pairs">Find candidate similar article pairs</h2>

<p>Write a function that uses the result of your LSH function and returns list of candidate article pairs. Spec:</p>

<ul>
<li>Input: the result of <code>do_lsh</code></li>
<li>Output: list of tuples <code>(docid1, docid2)</code> each a candidate similar article pair according to LSH</li>
</ul>

<h2 id="experiment-2-lsh-sensitivity">Experiment 2: LSH sensitivity</h2>

<p>Use these functions to compute the sensitivity and specificity of LSH as a function of the <code>threshold</code>. Use the 10,000 document dataset to perform this experiment.</p>

<h1 id="helpers">Helpers</h1>

<h2 id="obtaining-data">Obtaining data</h2>

<p>You can use the following python code to download data for the project</p>

<pre><code class="language-python">import os
from six.moves import urllib

DOWNLOAD_ROOT = &quot;https://raw.githubusercontent.com/chrisjmccormick/MinHash/master/data&quot;
PLAGIARISM_PATH = &quot;datasets/plagiarism&quot;
DATA_SIZES = [100,1000,2500,10000]

def fetch_data(download_root=DOWNLOAD_ROOT, 
               plagiarism_path=PLAGIARISM_PATH, 
               data_sizes=DATA_SIZES,
               maxsize=1000):
  if not os.path.isdir(plagiarism_path):
      os.makedirs(plagiarism_path)
  for size in data_sizes:
      if size &lt;= maxsize:
          train_file = &quot;articles_&quot; + str(size) + &quot;.train&quot; 
          train_path = plagiarism_path + '/' + train_file
          if not os.path.exists(train_path):
              train_url = download_root + '/' + train_file
              urllib.request.urlretrieve(train_url, train_path)
          
          truth_file = &quot;articles_&quot; + str(size) + &quot;.truth&quot;
          truth_path = plagiarism_path + '/' + truth_file
          if not os.path.exists(truth_path):
              truth_url = download_root + &quot;/&quot; + truth_file
              urllib.request.urlretrieve(truth_url, truth_path)
</code></pre>

<p>Using the function <code>fetch_data</code> will download data to a subdirectory pointed to by the variable <code>PLAGIARISM_PATH</code>. You can assign
the path you want to use for your data to taht variable. The <code>maxsize</code> argument of the <code>fetch_data</code> function is used to limit the size of data downloaded. For testing and development you should use the 1000 document dataset. You can get that with function call <code>fetch_data(maxsize=1000)</code>.</p>

<h2 id="generating-random-hash-functions">Generating random hash functions</h2>

<p>This function generates a random hash function suitable to mimic permutations over 32-bit integers. Recall since we are using <code>crc32</code> to represent items
we need random hash functions that generate 32-bit numbers.</p>

<pre><code class="language-python">import random

def make_random_hash_fn(p=2**33-355, m=4294967295):
    a = random.randint(1,p-1)
    b = random.randint(0, p-1)
    return lambda x: ((a * x + b) % p) % m
</code></pre>

<p>This is an example of how to use this function:</p>

<pre><code class="language-python">hash_fn = make_random_hash_fn()
hash_fn(12345)
</code></pre>

<p>Some notes: this implements a universal hash function for 32-bit integers, which will ensure the result corresponds to a permutation of rows of the
characteristic matrix as required by Minhash (see <a href="https://en.wikipedia.org/wiki/Universal_hashing">https://en.wikipedia.org/wiki/Universal_hashing</a>). Here <code>m</code> is the largest number returned by crc32, and <code>p</code>
is a prime number larger than <code>m</code>.</p>

<h2 id="comparing-numpy-vectors">Comparing numpy vectors</h2>

<p>The following bit of code shows how to use numpy to compare two integer vectors as required to compute the minhash similarity estimate.</p>

<pre><code class="language-python"># generate two vectors of length 50 with random integers from 0 to 100
a = np.random.randint(100, size=50)
b = np.random.randint(100, size=50)

# compute the fraction of entries in which two vectors are equal
np.mean(a == b)
</code></pre>

<h2 id="choosing-the-number-of-bands-for-lsh">Choosing the number of bands for LSH</h2>

<p>Given a similarity threshold, we need to choose the number of bands to use in LSH. Use this function to do this:</p>

<pre><code class="language-python">import scipy.optimize as opt
import math

def _choose_nbands(threshold, nhashes):
    error_fun = lambda x: (threshold-((1/x[0])**(x[0]/nhashes)))**2
    res = opt.minimize(error_fun, x0=(10), method='Nelder-Mead')
    b = int(math.ceil(res['x'][0]))
    r = int(nhashes / b)
    final_t = (1/b)**(1/r)
    return b, final_t
</code></pre>

<h2 id="hashing-a-vector">Hashing a vector</h2>

<p>In LSH for each band we hash the r hash values for each document. We can use this function to generate a hash function for vectors</p>

<pre><code class="language-python">def _make_vector_hash(num_hashes, m=4294967295):
    hash_fns = make_hashes(num_hashes)
    def _f(vec):
      acc = 0
      for i in range(len(vec)):
        h = hash_fns[i]
        acc += h(vec[i])
        return acc % m
    return _f
</code></pre>


			<aside class="copyright" role="note">
				
				&copy; 2018 Released under the MIT license &ndash;
				
				Documentation built with
				<a href="https://www.gohugo.io" target="_blank">Hugo</a>
				using the
				<a href="http://github.com/digitalcraftsman/hugo-material-docs" target="_blank">Material</a> theme.
			</aside>

			<footer class="footer">
				
			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'http:\/\/www.hcbravo.org\/dscert-mldm\/';
      var repo_id  = 'hcorrada\/dscert-mldm.git';
    
    </script>

    <script src="http://www.hcbravo.org/dscert-mldm/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

